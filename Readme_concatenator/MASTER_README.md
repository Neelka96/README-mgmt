# Crowdfunding ETL
`Project 2`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
By:  
&nbsp;&nbsp;&nbsp;&nbsp;**Manny Guevara**  
&nbsp;&nbsp;&nbsp;&nbsp;**Neel Agarwal**  
&nbsp;&nbsp;&nbsp;&nbsp;**Rob LaPreze**  
&nbsp;&nbsp;&nbsp;&nbsp;**Samora Machel**  

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [Deliverables](#deliverables)
    - [Required Files](#required-files)
    - [Optional Enhancements](#optional-enhancements)  
    - [RAW_autoSchema.sql Clarification](#raw_autoschemasql-clarification)
3. [Directory Structure](#directory-structure)
4. [System Requirements](#system-requirements)
5. [Installation & Setup](#installation--setup)
6. [ETL Process](#etl-process)
    - [Extraction](#1-extraction)
    - [Transformation](#2-transformation)
    - [Loading](#3-loading)
7. [Schema Creation & Data Import](#schema-creation--data-import)
    - [Example Table Creation](#example-table-creation)
    - [Example Import](#example-import)
8. [Regex Approach (Optional)](#regex-approach-optional)
9. [ERD Overview](#erd-overview)
    - [Normalization](#normalization)
    - [Relationships](#relationships)
10. [Limitations](#limitations)  
11. [Future Work](#future-work)
12. [Usage Notes & Tips](#usage-notes--tips)
13. [Credits & Citations](#credits--citations)

---

## Project Overview  

This project is designed to showcase an end-to-end **Extract, Transform, and Load (ETL)** process for crowdfunding campaign data as part of the edX/2U Data Analytics Bootcamp.

The objectives of this project are:
1. **Extract** crowdfunding data from provided Excel spreadsheets.
2. **Transform** and clean the data into a relational database-friendly format.
3. **Load** the cleaned data into a PostgreSQL database using SQL scripts.

The project emphasizes reproducibility and scalability by providing optional automation through notebook-driven SQL generation, all while fulfilling the assignment rubric. It avoids GUI-based imports (like pgAdmin's CSV import tool) to maintain cross-environment compatibility, even though doing so would be a reasonable option as well.

---

## Deliverables  
### Required Files  
According to the project rubric, the following are required:  
- **`ETL_FINAL.ipynb`**: The main notebook that handles extraction, transformation, and CSV creation.  
- **`crowdfunding_db_schema.sql`**: Defines the database schema for the transformed data. Rough draft generated by `schema_writer.ipynb`.
- **`ERD.jpg`**: Visual representation of the Entity Relationship Diagram (ERD) of the database.  

### Optional Enhancements  
- **`schema_writer.ipynb`**: Automatically generates rough-draft SQL schema and fully-functional import statements to bulk upload CSV Files
- **`crowdfunding_db_import.sql`**: Optional script to bulk-load CSVs into the database using `COPY` commands. <ins>Not part of repo, but generated SQL code that comes from `schema_writer.ipynb` when all cells are run.</ins>  

### RAW_autoSchema.sql Clarification  
**RAW_autoSchema.sql**:  
*(Generated, NOT FOR USE)* – Referring to the below directory structure, this file represents an unrefined, preliminary version of the schema automatically produced before manual corrections. It lacks necessary constraints, proper `VARCHAR` lengths, and tailored adjustments. It remains in the repository for historical and development reference only.  

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Directory Structure

```plaintext
Crowdfunding_ETL/
│
├── ETL/
│   ├── Deliverables/
│   │   ├── crowdfunding_db_schema.sql (Required)
│   │   └── ETL_FINAL.ipynb (Required)
│   └── Extras/
│       ├── crowdfunding_db_import.sql (Generated/Optional)
│       ├── RAW_autoSchema.sql (Generated/NOT FOR USE)
│       └── schema_writer.ipynb (Optional)
│
├── Resources/
│   ├── Input/
│   │   ├── contacts.xlsx
│   │   └── crowdfunding.xlsx
│   └── Output/
│       ├── campaign.csv
│       ├── category.csv
│       ├── contacts.csv
│       └── subcategory.csv
│
├── .gitignore
├── ERD.jpg (Required)
└── README.md
```

[:arrow_up: Return to TOC](#table-of-contents)  

---

## System Requirements

To successfully run this project, the following environment is recommended:

- **Operating System**: macOS, Windows, or Linux
- **Python Version**: 3.9 or higher
- **PostgreSQL**: Version 13 or higher
- **Required Python Libraries**:
    - `pandas`
    - `numpy`
    - `pathlib` (built-in)
    - `re` (built-in)
    - `json` (built-in)
    - `datetime` (built-in)
- **Jupyter Notebook**: For running `.ipynb` files (install via `pip install notebook` if needed)
- **PostgreSQL GUI (optional)**: pgAdmin 4 or any compatible interface
- **Disk Space**: Minimum 500 MB free
- **Memory (RAM)**: Minimum 4 GB recommended

Ensure PostgreSQL is installed and running before executing the SQL scripts.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Installation & Setup

1. **Clone or download** this repository.
2. Ensure you have **Python 3.x** installed with **pandas**:
    ```bash
    pip install pandas numpy 
    # You should have ipykernel already, if not, install it
    pip install ipykernel
    ```
3. (Optional) Set up a virtual environment:
    ```bash
    cd YOUR/PATH/TO/REPO/HERE       # Move to your local repo

    python -m venv venv             # Create a new virtual env

    source venv/bin/activate        # Mac/Linux or with
    venv\Scripts\activate           # Windows

    pip install requirements.txt    # Install logged dependencies

    deactivate                      # Deactivate your virtual env
    ```
4. Now you're ready to move to the Jupyter Notebook view!

> [!TIP]  
> As you might've gathered, a helper filer to automate SQL code was written. It has the ability to generate a SQL file to bulk-upload CSV type files. Feel free to upload in any way that is easiest, but to create this file please run all cells in `schema_writer.ipynb`.  

[:arrow_up: Return to TOC](#table-of-contents)  

---

## ETL Process

### 1. Extraction
- Decide on most efficient python based ETL tool.
- Install dependencies.
- Setup DataFrames so large rows of data can be read on-screen.
- Read in `crowdfunding.xlsx` and `contacts.xlsx` as DataFrames.  
- Quick look at data values, types, and structure before transformation to grasp what kind of database is being built. 

### 2. Transformation
Create four CSVs: `campaign.csv`, `contacts.csv`, `category.csv`, `subcategory.csv` by cleaning, editing, and normalizing data.  
- Split `"category & sub-category"` into separate `"category"` and `"subcategory"`:
    + `crowdfunding.xlsx` hold combined info that needed to be methodically separated
    + Create new identifiers for each entry in new tables to be linked with main table
    + Move into one new table for each along with unique identifiers
- Clean the `crowdfunding` (main) DataFrame and perform major edits:  
    + removing unnecessary sets from tables  
    + renaming/reordering columns to be more informative and match conventions  
    + merging to insert foreign key designations
    + standardizing data types for pandas ETL and SQL Schema  
- Mitigate bad input formatting to allow `contacts.xlsx` to be usable  
    + Select one of two methods to parse through the DataFrame created from `contacts.xlsx` (Pandas or Regular Expressions)
    + While both methods were completed for learning's sake and to measure efficiency, the Pandas method was selected for said reasoning.
    + Original Data was stored incorrectly using dictionary i.e. {} curly-brackets within excel, rendering the data unreadable by normal means.


### 3. Loading
- Create/connect to `crowdfunding_db` in PostgreSQL
- Add Schema into db using `crowdfunding_db_schema.sql`.
- Load data into db:
    - Using PostgreSQL's importing tool in the pgAdmin GUI
    - Using psql
    - Open `schema_writer.ipynb` and run all cells. Afterwards a `crowdfunding_db_import.sql` file can be found which can bulk-import CSV when run in pgAdmin 4.

While the basic requirements for this Project specify the inclusion of a schema file, the process of converting a pandas DataFrame into PostgreSQL Schema can be arduous and error-prone, so a helper notebook file was written to do most of the heavy lifting and data validation. Constraint edting, rounding up VARCHARs, and fixing DATE type in particular were done after the automatic building of the schema file. However, the bulk-imports that come from this helper file are fine to use if not even easier!

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Schema Creation & Data Import  

> [!NOTE]  
> The `VARCHAR` lengths for variable fields (like `first_name`, `last_name`, `email`, `company_name`, and `description`) were determined by the maximum string lengths from the dataset. These lengths were **manually rounded up** to accommodate longer entries in future datasets, preventing any data truncation issues.  

### Example Table Creation  

#### RAW_autoSchema
Note the lack of Primary and Foreign Key declarations as well as correcting `description` to be enclosed with double-quotes ("") to ensure it's read properly as a column name and not as a keyword. This is an example of the auto-generated SQL code created as a proof-of-concept, to speed up schema creation, and to ensure it's done with errors. For more information on Raw_autoSchema please [click here](#raw_autoschemasql-clarification).  
```sql
CREATE TABLE campaign (
    cf_id INT
    ,contact_id INT 
    ,company_name VARCHAR(33) NOT NULL
    ,description VARCHAR(53) 
    ,goal DEC NOT NULL
    ,pledged DEC NOT NULL
    ,outcome VARCHAR(10) NOT NULL
    ,backers_count INT NOT NULL
    ,country CHAR(2) NOT NULL
    ,currency CHAR(3) NOT NULL
    ,launch_date CHAR(10) NOT NULL
    ,end_date CHAR(10) 
    ,category_id CHAR(4) 
    ,subcategory_id VARCHAR(8) 
);
```

#### crowdfunding_db_schema
This is a snipped of the finalized schema code, after final edits have taken place. Keys are now declared properly, some VARCHARS have been rounded-up (albeit they could've been rounded up even more since they're personal identification materials), and `launch_date` and `end_date` have been changed to PostgreSQL DATE type.  
```sql
CREATE TABLE campaign (
    cf_id INT
    ,contact_id INT 
    ,company_name VARCHAR(40) NOT NULL
    ,"description" VARCHAR(60)
    ,goal DEC NOT NULL
    ,pledged DEC NOT NULL
    ,outcome VARCHAR(10) NOT NULL
    ,backers_count INT NOT NULL
    ,country CHAR(2) NOT NULL
    ,currency CHAR(3) NOT NULL
    ,launch_date DATE NOT NULL
    ,end_date DATE
    ,category_id CHAR(4) 
    ,subcategory_id VARCHAR(8) 
    ,CONSTRAINT pk_cf_id 
        PRIMARY KEY (cf_id)
    ,CONSTRAINT fk_contact_id 
        FOREIGN KEY (contact_id) 
        REFERENCES contacts(contact_id)
    ,CONSTRAINT fk_cat_id 
        FOREIGN KEY (category_id) 
        REFERENCES category(category_id)
    ,CONSTRAINT fk_subcat_id 
        FOREIGN KEY (subcategory_id) 
        REFERENCES subcategory(subcategory_id)
);
```

### Example Import
> [!NOTE]  
> Importing can be happily done using the GUI, but other options have been provided! Please run all cells in `schema_writer.ipynb` for a auto generated SQL Import file called `crowdfunding_db_import.sql`.

#### Using Bulk-Imports
```sql
COPY 
    campaign
FROM 
    '/your/local/path/Resources/Output/campaign.csv'
DELIMITER ',' CSV HEADER;
```

#### Using PSQL/SQL Shell
```sql
-- Optional:
\i crowdfunding_db_schema.sql
\i crowdfunding_db_import.sql
```

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Regex Approach (Optional)

An alternative method explored involved using **regular expressions** to extract fields from semi-structured text. This was ultimately replaced by pandas-based logic for efficiency.

### Example Regex Parsing
```python
import re

pattern_id = r'\d{4}'
pattern_email = r'"email": "(.+)"'
pattern_name = r'(\w+\s\w+)'

for row in contact_info_df['raw_data_column']:
    found_id = re.search(pattern_id, row).group()
    found_email = re.search(pattern_email, row).group(1)
    found_name = re.search(pattern_name, row).group()
```

[:arrow_up: Return to TOC](#table-of-contents)  

---

## ERD Overview

<img src="ERD.jpg" alt="ERD_JPEG" width="1000"/>

### Normalization:

This database schema follows **normalization** principles to minimize redundancy, maintain data integrity, and ensure scalability. By splitting data into tables like `category`, `subcategory`, `contacts`, and `campaign`, we:
- Reduce repeated data entries.
- Simplify updates to shared information.
- Improve database performance.

For example:
- Categories and subcategories are separated to prevent repeated text entries in multiple campaigns.
- Contacts are stored independently to link multiple campaigns to a single contact entry.

### Relationships:
- `campaign.contact_id` → `contacts.contact_id`
- `campaign.category_id` → `category.category_id`
- `campaign.subcategory_id` → `subcategory.subcategory_id`

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Limitations

- **`VARCHAR` Length Sensitivity**:  
    While the maximum observed string lengths in the dataset were manually rounded up to define `VARCHAR` limits for fields like `first_name`, `last_name`, `email`, `company_name`, and `description`, future datasets with significantly longer values may require further schema modifications to avoid truncation.

- **Manual Schema Refinements**:  
    Although the `schema_writer.ipynb` automates SQL generation, elements like primary and foreign keys, certain constraints like `NOT NULL`, and buffers for VARCHAR were applied manually or outside the automated process. Future iterations could integrate these features directly into the schema generator.

- **Limited Data Validation**:  
    The ETL process performs cleaning and transformation, but deeper validation (such as checking for duplicate records, malformed emails, or invalid date ranges) is minimal. Adding robust data validation steps could help ensure higher data quality.

- **Static File Paths**:  
    The generated import SQL file includes file paths that may need to be manually adjusted depending on the user's environment. This may limit portability across systems unless dynamic path handling is integrated.

- **Regex Solution Not Integrated**:  
    While an alternative regex-based contact parser was created, it remains outside the main workflow. Incorporating it as a selectable option within the ETL pipeline could provide flexibility for future datasets with more irregular formatting.

- **Scalability**:  
    This project was developed with relatively small sample datasets. Performance with much larger datasets (millions of records) has not been tested and may require optimization of memory handling and database indexing.

- **Normalization Trade-offs**:  
    The schema is normalized to reduce redundancy, but highly normalized schemas can sometimes lead to more complex queries and slower performance in certain analytics workflows. Denormalization strategies may be considered depending on usage needs.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Future Work  

- Automate the inclusion of schema constraints (primary/foreign keys, uniqueness) directly within `schema_writer.ipynb`.
- Introduce enhanced validation during the ETL process (such as email pattern checks, duplicate detection, and date validations).
- Add support for dynamic file path handling in the SQL import script to improve cross-system portability.
- Explore potential denormalization strategies to optimize read-heavy workloads and large-scale reporting.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Usage Notes & Tips

- **Path Fixes**: You can update file paths if moving files, however files are organized for clean runs
- **Manual Editing**: You can manually revise `crowdfunding_db_schema.sql` if you need additional constraints like `PRIMARY KEY` or `FOREIGN KEY`.
- **Enhancement**: Run `schema_writer.ipynb` for easy PostgreSQL file uploading, ignore the extra `RAW_autoSchema.sql` file that comes with it.
- **Optional Tools**: The `schema_writer.ipynb` and the import script are purely optional. They demonstrate an automated approach but aren’t required per the rubric.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Credits & Citations

1. **Collaborators**:  
    - *Manny Guevara*  
    - *Neel Agarwal*  
    - *Rob LaPreze*  
    - *Samora Machel*  
2. **Starter Code & Data**: Provided by **UT/edX/2U Data Analytics Bootcamp**.
3. **README.md**: Created using OpenAI's [ChatGPT LLM](https://www.chatgpt.com), trained using prior READMEs, all the deliverables, and the provided rubric given by edX/2U  
4. **PostgreSQL Docs**: [postgresql.org/docs](https://www.postgresql.org/docs/)
5. **pandas Docs**: [pandas.pydata.org/docs](https://pandas.pydata.org/docs)

[:arrow_up: Return to TOC](#table-of-contents)  

---
#EOF

# CurryScorer: Data Visualization & ETL Project
`Project 3`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  


## Overview

**CurryScorer** is designed to extract, transform, and visualize data for a specified domain (e.g., sports analytics, financial trends). The project encapsulates the ETL process and the backend for an interactive dashboard in a unified framework. By simply running the main script, the ETL pipeline is instantiated and executed, and the backend is set up to serve visualizations via a Flask web interface.

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Installation & Setup](#installation--setup)
3. [Technology Requirements](#technology-requirements)
    - [Operating Systems](#operating-systems)
    - [Python Dependencies](#python-dependencies)
4. [Directory Structure & File Breakdown](#directory-structure--file-breakdown)
5. [Deployment Details](#deployment-details)
6. [Database & ORM Layer](#database--orm-layer)
7. [Usage](#usage)
8. [Limitations](#limitations)
9. [Credits & Citations](#credits--citations)

---

## Project Overview

CurryScorer processes, analyzes, and visualizes data for [insert domain-specific purpose, e.g., sports analytics or financial trends]. The project is structured to allow a seamless data flow:
- **ETL Pipeline**: Instantiated and executed directly from the main script.
- **Visualization Engine**: Generates interactive and static charts.
- **Unified Execution**: A single command (`python app.py`) launches the entire process, making it simple to run in both development and production environments.


*Feel free to delete the database that comes with the cloned repo! Just be sure to include your own `NYC_OPEN_KEY` in the `.env` file!*

> [!WARNING]  
> For local execution, please make sure to include your own `.env` file in the root/ with the lines:  
>   `ENV = 'development'` and  
>   `NYC_OPEN_KEY = <yourKeyHere>`  
> Additionally, please check to make sure the configuration paths are valid for your system.

---

## Installation & Setup

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/CurryScorer.git
   cd CurryScorer
   ```

2. **Set Up Virtual Environment (Recommended)**

   - **Using venv (Python built-in):**

     ```bash
     python -m venv venv
     source venv/bin/activate  # On Windows: venv\Scripts\activate
     ```

   - **Using Conda:**

     ```bash
     conda create --name curryscorer_env python=3.12
     conda activate curryscorer_env
     ```

3. **Install Dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Application**

   Activate your virtual environment and start the project with:
   ```bash
   python app.py
   ```
   The main script instantiates the ETL pipeline and calls `.run()`, handling the rest of the process automatically.

---

## Technology Requirements

### Operating Systems

- **Supported OS**: Windows 10 or later, macOS Catalina or later, and modern Linux distributions.
- **Minimum Hardware**:
  - CPU: Dual-core processor or higher
  - Memory: 4 GB RAM (8 GB recommended)
  - Disk Space: 500 MB free (project files and dependencies)

### Python Dependencies

- **Python Version**: 3.8 or higher (tested on Python 3.12)
- **Key Libraries**:
  - `pandas`
  - `requests`
  - `Flask`
  - `Flask_CORS`
  - `SQLAlchemy`
  - `Tenacity`
  - Other dependencies as specified in `requirements.txt`

---

## Directory Structure & File Breakdown

Below is the tree diagram of the project’s structure along with a brief explanation of each component:

```plaintext
CurryScorer/
│
├── Core/
│   ├── resources/
│   │   ├── census_population.csv   # (REQUIRED FOR NOW) Population data file read for reading in.
│   │   ├── courier_dev.sqlite      # (Optional) SQLite Database that is used for this project, created if not present.
│   │   └── fastfood.csv            # (Optional) List of fast food restaurants, created if not present.
│   │
│   ├── backend/                    # Module for creating backend of Dashboard
│   │   ├── templates/              # Flask Templates for deploying HTML
│   │   │   └── home.html           # Only file here currently - creates pretty home route
│   │   ├── init.py                 # Top level of backend - Flask App lives here
│   │   └── backend.py              # Backend Helper
│   │
│   ├── etl/
│   │   ├── extract/                # MODULE - Extracting data from source files.
│   │   │   ├── init.py             # MODULE - Holds select dataset retrieval methods
│   │   │   └── extract.py          # Extract Helper
│   │   ├── init.py                 # BLANK - For library creation
│   │   ├── transform.py            # MODULE - Cleaning and normalizing data.
│   │   └── load.py                 # MODULE - Loading data into a usable format.
│   │
│   ├── init.py                     # MODULE - Pipeline Class creation. Manages ETL process.
│   ├── database.py                 # MODULE - Holds database schema and custom session management
│   └── log_config.py               # MODULE - Configured logger function for threading through project
│
├── frontend/
│   └── js/                         # Javascript for import to index.html
│       └── logic.js
├── index.html                      # Index html 
├── app.py                          # Main script to instantiate and run the pipeline.
├── .env                            # Important: required for environmental variables
├── requirements.txt                # List of Python dependencies.
└── README.md                       # This README file.
```

### Explanation of File & Directory Connections

- **Core/resources/:**
Contains essential data files. The required census_population.csv is used for data ingestion, while optional files like courier_dev.sqlite and fastfood.csv support extended functionalities. For a different experience, feel free to delete the SQLite DataBase that is included in the `Core/resources` of this repository when cloned! If that's the case the `.env` API `NYC_OPEN_KEY` is even more important to be included in the root of this cloned repo when running as it won't be able to proceed without it.

- **Core/backend/:**
Houses the Flask backend components. The templates/ directory contains HTML templates (currently just home.html), while init.py and backend.py set up and manage the backend service.  

- **Core/etl/:**
Encapsulates the ETL process:  

  - **extract/:** Retrieves raw data using methods defined in init.py and helper functions in extract.py.  

  - **transform.py**: Cleans and normalizes the extracted data.  

  - **load.py:** Loads the transformed data into the appropriate format (e.g., databases or CSV files).  

  - The parent **init.py** in the **Core** directory initializes the Pipeline Class that orchestrates the ETL process.  

- **Core/database.py:**
  Contains the database schema definitions and manages custom session handling for database operations.  

- **Core/log_config.py:**
Provides a configured logging function to ensure consistent logging throughout the project, useful for both debugging and production monitoring.  

- **frontend/:**
Contains the user-facing components. Currently only JavaScript. The files in js/ support interactive elements.  

- **index.html:**
The main HTML file for the frontend interface, included in the root for deployment to Github pages.  

- **app.py:**
Serves as the unified entry point. When run, it instantiates the ETL pipeline (by creating an instance of the Pipeline Class defined in Core/init.py) and calls its .run() method. This script handles both local execution and production deployment seamlessly.  

- **.env:**
Lists required environmental variables for runtime to succeed. Required variables are `ENV = development` for local execution and a `NYC_OPEN_KEY = <yourKeyHere>`

- **requirements.txt:**
Lists all Python dependencies to ensure consistent setup across environments.  

---  

## Deployment Details
**Backend Deployment:**  
The backend is deployed live to Azure at https://curryscorer.azurewebsites.net. The Flask app hosted on Azure serves the dashboard using the components in Core/backend/.

**Frontend Deployment:**  
The frontend is hosted via GitHub Pages, with index.html and associated JavaScript files in the frontend/ directory providing a clean and responsive user interface.

**Database Sharing & Concurrency Control:**  
To ensure that all instances of the web app use a single instance of the database, a file share in an Azure Storage Account is linked to the web app. This integration guarantees a centralized courier_dev.sqlite database even when the web app scales across multiple instances. Future enhancements will include implementing a locking mechanism to prevent multiple simultaneous updates, ensuring data integrity during concurrent operations.  

---  


## Database & ORM Layer
CurryScorer leverages SQLAlchemy 2.0 ORM to manage all interactions with the database. Key measures include:

- Schema Definition and Session Management:
The database schema is defined in Core/database.py and is built exclusively using SQLAlchemy 2.0 ORM. This module also includes custom session management to ensure that all database interactions—whether for building the database or executing queries—are conducted within the ORM’s context.

- Querying and Transactional Consistency:
All queries and data modifications are handled via SQLAlchemy’s ORM layer, ensuring that transactions are managed with the latest best practices for reliability and performance. This design promotes clean and maintainable code, reducing the risk of SQL injection and other common database pitfalls.

- Contextual Session Handling:
SQLAlchemy’s context managers are used to guarantee that sessions are properly closed after operations, ensuring that the database remains consistent and that resource usage is optimized across both development and production environments.


---

## Usage

**Running the Pipeline**:  
  With your virtual environment activated, your `.env` file setup, and configuration confirmed, simply execute:
  ```bash
  python app.py
  ```
  This command:

- Instantiates the ETL pipeline and executes the data extraction, transformation, and loading steps.

- Sets up the Flask backend to serve the dashboard, making it accessible via the live Azure URL.

- Ensures that both the backend and frontend deployments are in sync with the central database file share.

---

## Limitations

- **Manual Data Dependency:**  
  + The Extraction phase currently doesn't have a siphon for live census data, so instead manually extraction was leveraged to build the necessary CSV file.
  + Cuisine constants are currently hard coded in configuration, later updates should include a path for extracting what are actually considered to be "ethnic cuisines".

- **Data Volume:**  
  The ETL pipeline is optimized for moderate-sized datasets. Extremely large datasets might require further optimization or integration with distributed processing tools.

- **Coupled ETL & Flask App:**
  The Flask App is currently dependent on the insurance provided by the ETL Pipeline's instantiation. Later updates will de-couple and schedule ETL as a separate task.

- **Manual Configuration:**  
  Some settings (e.g., file paths for raw and processed data) may need manual adjustments depending on your environment.

- **Error Handling:**  
  While the pipeline is designed to be robust, additional logging and error handling might be necessary for production-level deployments.

- **Python Dependency Upgrades:**  
  Future upgrades to dependencies should be tested thoroughly to ensure compatibility across modules.

- **Concurrency Control:**  
Currently, a file share is used to centralize the database among multiple instances. A more advanced locking mechanism is planned to prevent concurrent update issues.

---

## Credits & Citations

- **Project Contributors**:  
  - [Neel Agarwal](https://github.com/neelka96)
  - [Manny Guevara](https://github.com/MannyHub24)
  - [Rob LaPreze](https://github.com/rlapreze2)
  - [Samora Machel](https://github.com/samora90)

- **Key References & Documentation**:  
  - [New York City Open Data](https://opendata.cityofnewyork.us/)
    + [DOHMH DataSet](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data)
    + [FastFood Restaurants](https://data.cityofnewyork.us/Health/DOHMH-MenuStat-Historical-/qgc5-ecnb/about_data)
    + [Socrata/SODA Querying](https://dev.socrata.com/docs/queries/)
  - [Current Python Docs](https://docs.python.org/3/contents.html)
  - [pandas Documentation](https://pandas.pydata.org/docs/)
  - [Flask Documentation](https://flask.palletsprojects.com/)
  - [SQL Alchemy Documentation](https://docs.sqlalchemy.org/en/20/)
  - [Tenacity Documentation](https://tenacity.readthedocs.io/en/latest/)
  - README template and guidance inspired by previous projects and ChatGPT assistance.

- **Special Thanks**:  
  Appreciation to the UT Data Analytics and Visualization Bootcamp for the foundational materials and the project rubric that shaped the design of CurryScorer.


#EOF

# HavocMapper: FEMA Responses to Extreme Weather  

## Introduction  

HavocMapper is a data analysis project exploring FEMA's Disaster Declarations Summary dataset and using  
Geoapify's geocoding API to uncover insights into disaster responses across the United States. This  
project applies foundational Python, Pandas, Matplotlib, and API skills to clean, analyze, and visualize  
data trends. Learn more about FEMA's work on their [website](https://www.fema.gov/) and Geoapify's API [here](https://www.geoapify.com/).  


## Table of Contents  

- [Introduction](#introduction)  
- [Setup](#setup)  
  - [Quick-Run Instructions](#quick-run-instructions)  
  - [Notebook Execution Order](#notebook-execution-order)  
  - [Reading Notebooks Effectively](#reading-notebooks-effectively)  
- [General Overview](#general-overview)  
  - [Major Technologies Used](#major-technologies-used)  
  - [System Requirements](#system-requirements)  
    - [Hardware](#hardware)
    - [Software](#software)
    - [Manual Dependencies](#manual-dependencies)
- [Detailed Description](#detailed-description)  
  - [Workflow](#workflow)  
  - [Purpose and Goals](#purpose-and-goals)  
  - [Techniques Applied](#techniques-applied)  
  - [Preferred Viewing Method](#preferred-viewing-method)  
- [Key Challenges](#key-challenges)  
- [Directory Structures](#directory-structures)  
  - [Original Cloned Repo](#original-cloned-repo)  
  - [New Expected Structure](#new-expected-structure)  
- [Deliverables](#deliverables)  
  - [Visualizations](#visualizations)  
  - [Analyses](#analyses)  
  - [Saved Figures and Models](#saved-figures-and-models)  
  - [Google Slides Presentation](#google-slides-presentation)  
- [Contributors](#contributors)  
- [Data Sources and Citations](#data-sources-and-citations)  
- [Long Form Directory Structure](#long-form-directory-structure)


## Setup  

### Quick-Run Instructions  

1. Clone the repository in the local directory of your choosing with `cd your/path/here`  
    - Using the HTTPS web URL:  
        ```bash  
        git clone https://github.com/neelka96/HavocMapper.git  
        cd HavocMapper  
        ```  
    - Using a password protected SSH Key (if you have it enabled other wise use method above):  
        ```bash  
        git clone git@github.com:Neelka96/HavocMapper.git  
        cd HavocMapper  
        ```  
2. Create a virtual environment (optional but recommended)  
    - With Conda:  
        ```bash  
        conda create --name venv python=3.12.2  
        conda activate venv  
        ```  
    - With Built-in Python venv Library:  
        ```bash  
        python -m venv venv  
        source venv/bin/activate  # On Windows: venv\Scripts\activate  
        ```  
3. Install dependencies:  
    ```bash  
    pip install -r requirements.txt  
    ```  
4. Feel free to deactivate your virtual environment (conda or otherwise) after installing dependencies!  
    - With Conda:  
        ```bash  
        conda deactivate  
        ```  
    - With venv:  
        ```bash  
        deactivate  
        ```  
5. Add API keys in a file named `api_keys.py`:  
    ```python  
    key1 = "your_api_key_here"  
    key2 = "your_optional_second_key_here"
    key3 = "your_optional_third_key_here"
    ...
    # Add additional keys as needed  
    # Native setup utilizes up to 9 keys  
    # Must have 1 key with no upwards limit  
    ```  
6. When running notebooks, be sure to select the kernel/environment the dependencies were installed in.  
> [!NOTE]  
> While the native system all code was built on runs through the conda python interpreter   
> (Anaconda Distribution), all dependencies were loaded through pip to ensure cross-compatibility  
> with other Python interpreters. Anaconda's official statement is that they can not ensure the  
> resolution of dependency conflicts from packages installed through Pypi, BUT conda does naturally  
> house pip installed dependencies separately from each other when installed in different conda  
> environments. When creating a new environment from scratch, this process will almost always  
> function as intended!


### Notebook Execution Order  

1. Run `havoc_ETL.ipynb` to set up directory, prepare data and geocode locations.  
2. Follow with `havoc_analysis.ipynb` for visualizations and analyses, and export images.  

### Reading Notebooks Effectively  

- Use Jupyter Notebook's Outline feature (e.g., in the VS Code extension) to collapse and expand sections for better navigation.  
- Outputs and markdown are organized with headings and foldable cells for clarity.  
> [!TIP]
> If running cell by cell, collapse all grouped cells first and then expand and collapse as needed!  
> Markdown has been thoroughly labeled and grouped for enhanced traversal.  


## General Overview  

This project follows a structured workflow to explore and analyze FEMA's disaster declarations. Data is extracted,  
cleaned, geocoded, and visualized to identify trends in extreme weather responses, leveraging modern Python  
libraries and API integrations.  


### Major Technologies Used  

- Python 3.12.2  
- Jupyter Notebook  
- pandas  
- matplotlib  
- requests  


### System Requirements  

#### Hardware  

- A modern computer with at least 8GB RAM and 2GHz multi-core CPU.  
- Internet access for API calls.  

#### Software  

- Python 3.12 or higher  
- Recommended: Conda for environment management.  

#### Manual Dependencies 
If not using `requirements.txt` please pip install the following if you don't already have these libraries:  
- pandas  
- pathlib  
- matplotlib  
- holoviews  
- geoviews  
- hvplot  
- seaborn  
- dataframe\_image  


## Detailed Description  

### Workflow  
1. **Data Extraction**:  
  - Download and clean FEMA dataset.  
  - Apply minor and major cleaning steps, including regex operations and vector mappings.  
  - Sets up HavocMapper/assets directory  
2. **API Integration**:  
  - Geocode locations using Geoapify API.  
  - Utilize `ThreadPoolExecutor` for multi-threaded API calls.  
3. **Data Aggregation**:  
  - Summarize FEMA merged geocoded data via groupby to reduce data points.  
  - Export multiple intermediate and final CSV files.  
4. **Analysis and Visualization**:  
  - Load cleaned data for analysis.  
  - Analyze via different lenses of time, states, types of disasters, and response times.
  - Generate visualizations such as maps, line plots, and bar charts, and scatter plots.  

### Purpose and Goals  

- Provide insights into FEMA disaster responses over time.  
- Highlight the efficiency and accuracy of FEMA's dataset.  
- Identify trends and anomalies in disaster declarations.  

### Techniques Applied  

- Regex for text data cleaning.  
- Multi-threading for API efficiency.  
- Vectorized operations for improved performance.  
- Exploratory data analysis (EDA) with Pandas and Matplotlib.  

### Preferred Viewing Method  

- View notebooks in VS Code or Jupyter Notebook.  
- Check saved figures in the `assets/fig/` directory.  
- Review intermediate CSVs in `assets/csv/`.  


## Key Challenges  

1. Deciding on project goals and API usage.
2. Understanding and parsing FEMA's dataset.  
3. Handling API rate limits and multi-threading with a large dataset.  
4. Adapting workflow to handle large datasets efficiently.  
5. Creating efficient workflow that allowed everyone to contribute at the same time


## Directory Structures  

### Original Cloned Repo  

```  
HavocMapper/
|-- docs/
|-- havoc_ETL.ipynb
|-- havoc_analysis.ipynb
|-- requirements.txt
```  

### New Expected Structure  

```  
HavocMapper/
|-- assets/
|   |-- csv/
|   |   |-- raw/
|   |   |-- clean/
|   |-- fig/
```  


## Deliverables  

### Visualizations  

- Line plots, scatter plots, and bar charts.  
- Choropleth maps with Holoviews and Bokeh.  

### Analyses  

- FEMA disaster trends by state and time.  
- Severe weather/storm trends by years.  
- Cycle times and distributions of responses.  
- Geographic distribution of disaster events by type and disaster itself.

### Saved Figures and Models  

- Figures saved in the `assets/fig/` directory, organized by contributor.  

### Google Slides Presentation  

- Overview of findings, shared in PDF format in `docs/PDFs/`.  


## Contributors  

- **Alex**: Disaster frequencies and trends.  
- **Avenika**: Storm trends and summaries.  
- **Claudia**: Disaster cycles and state responses.  
- **Neel**: GitHub management, API integration, README creation.  


## Data Sources and Citations  

- [FEMA Disaster Declarations Summary Dataset](https://www.fema.gov/openfema-data-page/disaster-declarations-summaries-v2)  
- [Geoapify API](https://www.geoapify.com/)  


## Long Form Directory Structure

Structure of GitHub Repo:
```
HavocMapper/
|
|-- docs/
|   |-- Expected Assets/
|   |   |-- csv/
|   |   |   |-- clean/
|   |   |   |   |-- distilled_summary.csv
|   |   |   |   |-- femaDisasters_geocode.csv
|   |   |   |-- raw/
|   |   |   |   |-- DisastersDeclarationsSummaries.csv
|   |   |   |   |-- femaDisasters_rough.csv
|   |   |   |   |-- location_geocode.csv
|   |   |-- fig/
|   |   |   |-- alex/
|   |   |   |   |-- MostCommonDisasters_byTopStates.png
|   |   |   |   |-- Top10_MostFrequent_Disasters.png
|   |   |   |   |-- Top10_States_MostDisasters.png
|   |   |   |-- avenika/
|   |   |   |   |-- FloodTrend_byYear.png
|   |   |   |   |-- HurricaneTrend_byYear.png
|   |   |   |   |-- SevereStormTrend_byYear.png
|   |   |   |-- claudia/
|   |   |   |   |-- AvgCycleTime_byType.png
|   |   |   |   |-- CycleTimeDistr_byType.png
|   |   |   |   |-- DisasterCount_byState.png
|   |   |   |   |-- DisasterNumber_overTime.png
|   |   |   |   |-- DisasterNumbers_byYear.png
|   |   |   |   |-- TimeForDeclaration_byState.png
|   |   |   |   |-- Year_over_year_disasters.png
|   |   |   |-- neel/
|   |   |   |   |-- cleaned_df.png
|   |   |   |   |-- original_df.png
|   |   |   |   |-- summary_df.png
|   |-- Original Notebooks/
|   |   |-- Alex_Notebook.ipynb
|   |   |-- Avenika_Notebook.ipynb
|   |   |-- Claudia_Notebook.ipynb
|   |   |-- Neel_Notebook.ipynb
|   |-- PDFs/
|   |   |-- disaster_declaration_process.pdf
|   |   |-- FEMA Weather Impact Analysis.pdf
|   |   |-- InternationalCountryCodes.pdf
|   |-- fact_sheet.md
|- .gitignore
|- DisastersDeclarationsSummaries.csv
|- havoc_ETL.ipynb
|- havoc_analysis.ipynb
|- README.md
|- requirements.txt
```

Structure of Intended Output Directory with new assets directory - should mimic the Expected Assets directory:
```
HavocMapper/
|
|-- assets/
|   |-- csv/
|   |   |-- clean/
|   |   |   |-- distilled_summary.csv
|   |   |   |-- femaDisasters_geocode.csv
|   |   |-- raw/
|   |   |   |-- DisastersDeclarationsSummaries.csv
|   |   |   |-- femaDisasters_rough.csv
|   |   |   |-- location_geocode.csv
|   |-- fig/
|   |   |-- alex/
|   |   |   |-- MostCommonDisasters_byTopStates.png
|   |   |   |-- Top10_MostFrequent_Disasters.png
|   |   |   |-- Top10_States_MostDisasters.png
|   |   |-- avenika/
|   |   |   |-- FloodTrend_byYear.png
|   |   |   |-- HurricaneTrend_byYear.png
|   |   |   |-- SevereStormTrend_byYear.png
|   |   |-- claudia/
|   |   |   |-- AvgCycleTime_byType.png
|   |   |   |-- CycleTimeDistr_byType.png
|   |   |   |-- DisasterCount_byState.png
|   |   |   |-- DisasterNumber_overTime.png
|   |   |   |-- DisasterNumbers_byYear.png
|   |   |   |-- TimeForDeclaration_byState.png
|   |   |   |-- Year_over_year_disasters.png
|   |   |-- neel/
|   |   |   |-- cleaned_df.png
|   |   |   |-- original_df.png
|   |   |   |-- summary_df.png
|-- docs/
|   |-- Expected Assets/
|   |   |-- csv/
|   |   |   |-- clean/
|   |   |   |   |-- distilled_summary.csv
|   |   |   |   |-- femaDisasters_geocode.csv
|   |   |   |-- raw/
|   |   |   |   |-- DisastersDeclarationsSummaries.csv
|   |   |   |   |-- femaDisasters_rough.csv
|   |   |   |   |-- location_geocode.csv
|   |   |-- fig/
|   |   |   |-- alex/
|   |   |   |   |-- MostCommonDisasters_byTopStates.png
|   |   |   |   |-- Top10_MostFrequent_Disasters.png
|   |   |   |   |-- Top10_States_MostDisasters.png
|   |   |   |-- avenika/
|   |   |   |   |-- FloodTrend_byYear.png
|   |   |   |   |-- HurricaneTrend_byYear.png
|   |   |   |   |-- SevereStormTrend_byYear.png
|   |   |   |-- claudia/
|   |   |   |   |-- AvgCycleTime_byType.png
|   |   |   |   |-- CycleTimeDistr_byType.png
|   |   |   |   |-- DisasterCount_byState.png
|   |   |   |   |-- DisasterNumber_overTime.png
|   |   |   |   |-- DisasterNumbers_byYear.png
|   |   |   |   |-- TimeForDeclaration_byState.png
|   |   |   |   |-- Year_over_year_disasters.png
|   |   |   |-- neel/
|   |   |   |   |-- cleaned_df.png
|   |   |   |   |-- original_df.png
|   |   |   |   |-- summary_df.png
|   |-- Original Notebooks/
|   |   |-- Alex_Notebook.ipynb
|   |   |-- Avenika_Notebook.ipynb
|   |   |-- Claudia_Notebook.ipynb
|   |   |-- Neel_Notebook.ipynb
|   |-- PDFs/
|   |   |-- disaster_declaration_process.pdf
|   |   |-- FEMA Weather Impact Analysis.pdf
|   |   |-- InternationalCountryCodes.pdf
|   |-- fact_sheet.md
|- .gitignore
|- DisastersDeclarationsSummaries.csv
|- havoc_ETL.ipynb
|- havoc_analysis.ipynb
|- README.md
|- requirements.txt
```
#EOF

# Neelka96.github.io
Application to scrape my own github and create a dashboard

#EOF

# README: VBA-challenge
> Current Version: 2oStockRoutine.bas
> 
> Module Path: [VBA-challenge/2oStockRoutine.bas](/2oStockRoutine.bas)
> 
> **Last Updated: December 2nd, 2024**

> [!IMPORTANT]
> For the original Module written from only the information given in class using ws.Range() as an access point please see the [1oStockRoutine!](/1oStockRoutine.bas)
>
> New and improved version is [2oStockRoutine](/2oStockRoutine.bas) as stated before!

# Important Information
**Made for EdX & UT Data Analytics and Visualization Bootcamp: Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC.** 

Script is a macro written in VBA (Visual Basic for Applications) for use solely with Microsoft Excel files types and has only been tested with .xlsm and .xlsx file formats.

This is the second module completed in the course!

This README.md was written using tips and tricks from [GitHub Docs](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).

For all citations please see [VBA-challenge Citations](#vba-challenge-citations).

## Module Purpose
Module's purpose is to analyze a large amount of data held on multiple pages of a workbook. The data consists of daily stock values and the objective is to provide a summary for each one on each sheet, along with a smaller summary for each worksheet. After an examination of the data, the module creates new tables of data with corresponding formatting and headers that will be filled in with computations that have been run on each grouping of stock tickers to create the summaries. The workbook and any worksheets within it should only update after the module finishes its execution along with message boxes declaring the macro a success and lisitng the execution time.

## CURRENT METHOD:
```
<Run Macro>
--> Disable application update features
--> Begin loop through each worksheet
--> Copies data and perserves formatting/arrangement in same 2D array dimensions
--> Runs data through tests in local memory
--> Stores results in two new 2D arrays
--> Two new arrays with headings and formatting printed back to workshees
--> Loop to next worksheet
--> Success! Re-enable application update features to see changes
--> Print message box stating success and execution time
<End Macro>
```

## Detailed Description of Module
Built to work with daily entries of various stocks and their associated values listed in one or more worksheets. Suboutine takes the input of a workbook and copies pre-organized data within the workbook (and its dimensions) to the temporary memory of local computer in the form of a variant array. The array data include the stock name, date of entry, the opening, high, low, and closing prices, and volume for each entry. The sample data given by the class has the workbook seperated into quarters and the stocks grouped by tickers which makes comparing groups of stocks easier. 

Looping within the new array, it only pauses at new stock tickers to compute the statistics of the prior range of stock tickers. The module will summarize the data for each stock ticker group and working one quarter at a time until the output is completed. The computations are stored in two new 2D arrays on computer by grouping of stock ticker. After new arrays are built, data is printed back to each worksheet with macro-based conditional formatting. Finally, the module permits the Microsoft Excel UI to resume updates before subroutine end.

> [!NOTE]
> [Current subroutine](/2oStockRoutine.bas) is now built using arrays to handle large quantities of data with rapid efficiency! [Previous subroutine](/1oStockRoutine.bas) ran by continuously accessing Microsoft Excel Ranges in the workbook and storing all temporary data within the sheet, however it executed at only 20% of the speed of current mode.

## Current Limitations
- [ ] Runs only with Microsoft Excel Workbooks.
- [ ] Stock tickers don't have to be in alphabetical order, but they MUST be grouped by ticker name.
- [ ] Stock tickers in the same group MUST be in exact chronological order (Earliest date --> Latest Date).
- [ ] Input format must be: {Rows} x {Columns} --> {Ticker, date, open, high, low close, volume} x {Daily stock entries}
- [ ] Input data is NOT verified/formatted/cleansed by module - adding said functionality has not been addressed.


# VBA-challenge Citations
For writing [2oStockRoutine.bas](/2oStockRoutine.bas) source code and its supporting README.md documentation multiple sources were used acrossed the web including:
- [ChatGPT 1st Access](#chatgpt-access-1)
- [ChatGPT 2nd Access](#chatgpt-access-2)
- [Microsoft Learn](#microsoft-learn-guide)
- [GitHub Docs](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
- [And this university website](#readme-help-citations)

## Source Code Citations
<a name="chatgpt-access-1"></a>
1. ChatGPT Optimization Suggestion Using Application Object Properties [^1]
   - Desciption: ChatGPT's algorithm suggested use of setting Application.Property = Value temporarily turns off Excel Worksheet properties that run during macro use until subroutine is done modifying data to improve latency.
   - Author: OpenAI
   - Date: 2024
   - Code Version: ChatGPT-4o
   - Availability: https://www.chatgpt.com
[^1]: [ChatGPT-4o by OpenAI (2024)](https://www.chatgpt.com) used for optimization help:
  Suggested modifying module [1oStockRoutine.bas](/1oStockRoutine.bas) by setting Application.<Property> values
```
Application.ScreenUpdating = False
Application.Calculation = xlCalculationManual
Application.EnableEvents = False

<Subroutine Core Goes Here>

Application.ScreenUpdating = True
Application.Calculation = xlCalculationAutomatic
Application.EnableEvents = True
```
<a name="chatgpt-access-2"></a>
2. ChatGPT Optimization Suggestion Using Variant Arrays [^2]
   - Description: ChatGPT suggested use of RAM (variant type array method) for storing, accessing, computing, comparing, and printing data. Implemented for its speed.
   - Author: OpenAI
   - Date: 2024
   - Code Version: ChatGPT-4o
   - Availability: https://www.chatgpt.com
[^2]: [ChatGPT-4o by OpenAI (2024)](https://www.chatgpt.com) used for optimization help:
  Suggested modifying module [1oStockRoutine.bas](/1oStockRoutine.bas) to utilize arrays for faster processing
```
Dim stockInput As Variant   'Array will hold all original data (aka Input)
'Has dimensions of: {# Columns} x {# Rows} or more precisely...
'{Ticker, date, open, high, low close, volume} x {Distinct stock entry dates}
    
Dim stockOutput() As Variant   'Array will hold all output values - ReDim'd later at exact sizing
'Has dimensions of: {# Columns} x {# Rows} or more precisely...
'{Ticker, quarterly difference, quarterly percent difference, total volume} x {Distinct stock tickers}
    
Dim stockStats(1 To 3, 1 To 2) As Variant   'Array will hold largest values within stockOutput() for output also
'Has dimensions of: {# Columns} x {# Rows} or more precisely...
'{Ticker, value} x {Greatest % increase, greatest % decrease, greatest volume}
```
<a name="microsoft-learn-guide"></a>
3. Microsoft Learn VBA Reference Guide [^3]
   - Description: Information on UBound(), With, ReDim declaration, variant data type, and conditional formatting
   - Author: Microsoft Learn/@o365devx/@AlexJerabek/@kbrandl/@OfficeGSX/@Saisang
   - Date: 2021-2022
   - Availability: https://learn.microsoft.com/en-us/office/vba/api/overview/
[^3]: [Microsoft Learn VBA Reference Guide](https://learn.microsoft.com/en-us/office/vba/api/overview/) used for help with understanding built-in formula syntax:
   Formulas include UBound(), With var As Type, ReDim var As Type, and Dim var As Variant, and using Range.FormatConditions.Add()
```
For i = 2 To UBound(stockInput, 1)   'Looping from 2nd index to last index of stockInput()
      If stockInput(i, 1) <> stockInput(i - 1, 1) Then   'Checks if prior value is different
            outputCount = outputCount + 1
      End If
Next i   'NOTE: Starting at 1 and comparing to i + 1 returns bound error
ReDim stockOutput(1 To outputCount, 1 To 4)   'Reallocating stockOutput to exact size needed

...

Dim conditionRange As Range
Set conditionRange = .Range("J2:J" & j)   'Set variable = range needs formatting

With conditionRange   'Using "With" for ease of access
      .ClearFormats   'Clears any existing formatting - Zero values are blank
      .FormatConditions.Delete   'Clears any existing conditional formatting too
                
      With .FormatConditions.Add(xlCellValue, xlGreater, "=0.00").Interior   '% > 0 Set to green
            .ColorIndex = 4
      End With
      With .FormatConditions.Add(xlCellValue, xlLess, "=0.00").Interior   '% < 0 Set to red
            .ColorIndex = 3
      End With
      .NumberFormat = "#0.00"   'Set quarterly difference to 0.00 decimal placing
End With
```

## README Help Citations
1. Written by Gries, D., L. Lee, S. Marschner, and W. White (over the years), published in 2014, the page "Academic Integrity, CS 1110..." was published online with information on how to list citations within source code. [^4]
2. Published by GitHub for assistance using their markdown language in a README.md file! Includes information on hard/soft links, anchors, code embedding, picture sourcing and more! [^5]
[^4]: Gries, D., et al. "Academic Integrity, CS 1110: Introduction to Computing Using Python: Fall 2014." Pellissippi Community College State Libraries, Sept. 2014, lib.pstcc.edu/csplagiarism/citation. Accessed 29 Nov. 2024.
[^5]: GitHub, "Basic writing and formatting syntax" GitHub Docs, 2024, https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax. Accessed 29 Nov. 2024

## List of Subroutine SoftLinks
[Link to Subroutine 2o](/2oStockRoutine.bas)

[Link to Subroutine 1o](/1oStockRoutine.bas)

#EOF

# Belly Button Biodiversity Dashboard

**Module 14 Challenge**  
**EdX/UT Data Analytics and Visualization Bootcamp**  
**Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC**  
**Author:** Neel Agarwal

## Table of Contents
1. [Project Description](#project-description)
2. [Assignment Requirements](#assignment-requirements)
   - [Base Rubric](#base-rubric)
   - [Enhancements & Extra Features](#enhancements--extra-features)
3. [Directory Structure](#directory-structure)
   - [Directory Tree](#directory-tree)
   - [Key Files](#key-files)
4. [Usage Instructions](#usage-instructions)
   - [Technologies Used](#technologies-used)
5. [Key Functions & Code Notes](#key-functions--code-notes)
6. [GitHub Pages Deployment](#github-pages-deployment)  
7. [References and Citations](#references-and-citations)

---

## Project Description
This repository contains an interactive dashboard that explores the **Belly Button Biodiversity** dataset. The goal is to visualize the bacteria found in human belly buttons by subject ID, displaying both a bar chart of the top bacterial species and a bubble chart of all bacterial cultures. This assignment deviates from typical Python-focused projects by relying primarily on **JavaScript, HTML, and CSS** (plus libraries like **D3.js** and **Plotly.js**) to create a dynamic web-based visualization experience. While this project goes beyond the basic requirements for this module, it doesn't interfere with the fulfillment of any of those requirements in creating extra functionality.

---

## Assignment Requirements
The assignment's base requirements and extra features are separated and summarized in the section below!  


### Base Rubric
Below are the core requirements (as stated or implied by the Module 14 Challenge rubric) and how each one is addressed in this project:

1. **Create a Horizontal Bar Chart**  
   - **Rubric Requirement:** Display the top 10 (or more) bacterial species (OTUs) found in each volunteer’s navel.  
   - **Implementation:**  
     - Bar chart is generated in `app.js` by slicing and reversing the relevant data arrays (`otu_ids`, `sample_values`, `otu_labels`).  
     - The chart is rendered via Plotly to a `<div id="bar">`.  
   - **Location:** See `barChart(json, params)` function in `app.js`.

2. **Create a Bubble Chart**  
   - **Rubric Requirement:** Visualize the overall distribution of bacterial species for each individual.  
   - **Implementation:**
       - Bubble chart uses `otu_ids` for the x-axis, `sample_values` for the y-axis and marker sizes, and color scales the bubbles by `otu_ids`.  
       - Rendered via Plotly to `<div id="bubble">`.  
   - **Location:** See `bubbleChart(json, params)` function in `app.js`.

3. **Populate a Demographic Info Panel**  
   - **Rubric Requirement:** Display each volunteer’s demographic information (ID, age, location, etc.).  
   - **Implementation:**  
       - A `<div id="sample-metadata">` in `index.html` is updated dynamically using the `buildMetadata()` function.  
       - Data is filtered by the volunteer’s ID; key-value pairs are appended into that `<div>` for an at-a-glance summary.  
   - **Location:** See `buildMetadata(sampleNum, json)` in `app.js`.

4. **Dynamic Input/Dropdown**  
   - **Rubric Requirement:** Provide a dropdown to select volunteer IDs, and upon user change, redraw charts.  
   - **Implementation:**  
       - `<select id="selDataset" />` in `index.html` triggers `sampleChange(this.value)` in `app.js`.  
       - The data is retrieved/filtered, and the charts and panel are updated accordingly.  
   - **Location:** `buildDropDown()` and `sampleChange()` in `app.js`.

5. **README**  
   - **Rubric Requirement:** An updated README that describes the purpose, process, dependencies, usage instructions, etc.  
   - **Implementation:**  
       - This document provides an overview of all the above features, instructions, and references.


### Enhancements & Extra Features
In addition to the rubric requirements, the following enhancements go **beyond** the minimal scope:

1. **Random Subject Selector**  
   - A “Randomize” button that picks a random subject ID to demonstrate dynamic chart rendering.

2. **Dynamic Bar Limit**  
   - A custom dropdown (`id="barLimit"`) to let users choose how many bars to display (e.g., top 5, top 10, or “all”).

3. **Real-time Alerts/Warnings**  
   - If the data set for a particular subject ID is too small—or if you request more bars than available—the UI displays a warning message (`displayWarnings()` in `app.js`).

4. **Loading Spinner / Overlay**  
   - A loader UI that appears while fetching or updating data (`showLoader()` / `hideLoader()` in `app.js`).

5. **Color Change Interactions**  
   - Some elements dynamically change color or background on click, providing a more interactive and modern user experience.

6. **Optimized Code Structure**  
   - The code is split into smaller, reusable helper functions (e.g., `cleanLabels()`, `buildDropDown()`, etc.) for maintainability.  

7. **Regex Text Cleaning**
   - The demographic info is cleaned before presentation per each request for new data, to allow for consistent presentation and potential storage of data.

8. **More CSS Libraries**
   - Introduced CSS libraries to enhance existing Bootstrap styling and create favicon icons.


[:arrow_up: Return to TOC](#table-of-contents)  


---


## Directory Structure

### Directory Tree
Below is a simplified view of the project layout:

```
BellyButtonBiodiversity/
│
├── index.html
├── static/
│   └── js/
│       └── app.js
│
├── README.md
└── .gitignore
```

### Key Files  
- **`index.html`**  
   - Contains the webpage structure, layout with Bootstrap columns, dropdowns, and `<div>` placeholders for charts.  
- **`app.js`**  
   - The main JavaScript logic. Connects to the JSON data, populates dropdowns, updates charts, and displays the demographic info.  


[:arrow_up: Return to TOC](#table-of-contents)  


---

## Usage Instructions
1. **Clone or Download** this repository.  
2. **Locate `index.html`** in your local folder.  
3. **Open `index.html` in a browser** (Chrome/Firefox recommended).  
4. The page loads with a default test subject (or the first ID).  
5. **Use the dropdown** menu to pick a subject ID—charts and info update automatically.  
6. Optionally, click **“Subject ID Randomizer”** to see a random subject.  
7. Adjust the **“Number of Bars to Show”** dropdown to see fewer/more bar segments.

<h3>OR!</h3>  

1. Visit the GitHub Page to visit the tool preloaded in your web-browser by [clicking here!](https://neelka96.github.io/belly-button-challenge/)  
2. You can also explore more information about how this tool was set up in [this section](#github-pages-deployment) of the README!  

> [!NOTE]  
> If data is being fetched from a local `samples.json`, ensure that file is present in the correct relative path  
> or served via local server (some browsers block local JSON fetch for security).

### Technologies Used
- **JavaScript (ES6+)**  
- **HTML5 / CSS3**  
- **D3.js v7**  
- **Plotly.js**  
- **Bootstrap 5**  
- (Optional) **fetch / d3.json** for data loading  
- (Optional) **Bootstrap Bundle** for custom shading and boxing
- (Optional) **Awesome Font's Favicon CSS Library**  


[:arrow_up: Return to TOC](#table-of-contents)  


---

## Key Functions & Code Notes
- **`sampleChange(selectedID)`**  
   - Main orchestration function for updating charts and metadata each time the user changes the dropdown.  
- **`buildMetadata(sampleNum, data)`**  
   - Dynamically populates the demographic info panel.  
- **`barChart(jsonData, plotlyParams)` & `bubbleChart(jsonData, plotlyParams)`**  
   - Construct the respective Plotly visualizations with the chosen data arrays.  
- **`displayWarnings(total, requested, alertBoxID)`**  
   - Posts warnings/alerts if the user tries to show more bars than exist or if the sample size is too small.  
- **`showLoader()` / `hideLoader()`**  
   - Control the loading spinner overlay for a smoother UX.  

Feel free to explore these functions in `app.js` and adapt them for your future projects.  


[:arrow_up: Return to TOC](#table-of-contents)  


---

## GitHub Pages Deployment

This project is also hosted via **GitHub Pages**, allowing anyone to view the interactive dashboard directly in their browser without cloning or running a local server. Here’s how you can check it out—or set it up yourself if you haven’t already:

1. **View Live Dashboard**  
   - You can access the deployed dashboard at [https://neelka96.github.io/belly-button-challenge/](https://neelka96.github.io/belly-button-challenge/).  
   - Once there, you’ll see the homepage (`index.html`) loaded with the interactive charts.

2. **How to Deploy on GitHub Pages**  
   + **Push Code to GitHub**: Make sure your `index.html`, `app.js`, and other files are on your main branch.  
   + **Enable Pages**:  
      - Go to your repository’s **Settings** tab on GitHub.  
      - In the left-hand menu, click on **Pages**.  
      - Under **Source**, choose the branch you want to deploy (e.g., `main`) and the `/root` folder.  
      - Click **Save**.  
   + **Wait for Build**: It may take a minute or two for GitHub to build the site. Once successful, you’ll see a green bar with the URL of your hosted site.  
   + **Visit the Deployed Dashboard**: Click or navigate to the URL (e.g., `https://YourGitHubUsername.github.io/YourRepoName`) to see the live site.

3. **Updating the Deployment**  
   - Whenever you push additional commits to the chosen branch, GitHub automatically rebuilds and updates your Pages site.  
   - Just wait a minute or two for changes to propagate, then refresh your live URL.

4. **Notes**  
   - GitHub Pages only serves static content. Your JavaScript and data files should be relative paths in your HTML (e.g., `./static/js/app.js` or similar) to ensure everything loads correctly.  
   - If using local JSON or other data, ensure it’s included in the repository and references in `app.js` or HTML with a relative file path.  
   - If you run into CORS issues or “404 Not Found,” double-check spelling, capitalization, and folder organization.


[:arrow_up: Return to TOC](#table-of-contents)  


---

## References and Citations
- **edX/2U** Bootcamp materials for the “Belly Button Biodiversity” dataset and assignment instructions.  
- **README.md**: Created using OpenAI's [ChatGPT LLM](https://www.chatgpt.com), trained using prior READMEs, all the deliverables, and the provided rubric given by edX/2U  
- **Plotly.js Official Docs**: [https://plotly.com/javascript/](https://plotly.com/javascript/)  
- **D3.js** Documentation: [https://d3js.org/](https://d3js.org/)  
- **Bootstrap** Documentation: [https://getbootstrap.com/](https://getbootstrap.com/)  
- **Font Awesome** Documentation: [https://fontawesome.com/v4/](https://fontawesome.com/v4/)  
- **Random** code snippets for event handling, arrow functions, etc., derived from [MDN Web Docs](https://developer.mozilla.org/) and personal experience.  


[:arrow_up: Return to TOC](#table-of-contents)  
#EOF

# Leaflet Challenge

**Module 14 Challenge**  
**EdX/UT Data Analytics and Visualization Bootcamp**  
**Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC**  
**Author:** Neel Agarwal

**Leaflet Challenge** is a front-end project built with HTML, CSS, and JavaScript that showcases interactive mapping using the Leaflet library. The project is divided into two parts:

- **Part 1:** The core mapping challenge with essential functionality.
- **Part 2 (Optional):** An extended version with additional features and enhancements. A duplicate `index.html` is provided in the root of the repository to facilitate hosting via GitHub Pages.

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Installation & Setup](#installation--setup)
3. [Technology Requirements](#technology-requirements)
4. [Directory Structure & File Breakdown](#directory-structure--file-breakdown)
5. [Deployment Details](#deployment-details)
6. [Usage](#usage)
7. [Limitations](#limitations)
8. [Credits & Citations](#credits--citations)

---

## Project Overview

Leaflet Challenge demonstrates interactive mapping capabilities using Leaflet.js. It provides a rich visual experience with dynamic data overlays, custom markers, and interactive layers. The project is designed for both learning and showcasing modern web mapping techniques.

- **Part 1:** Implements the basic mapping features and data visualization.
- **Part 2 (Optional):** Expands on the base functionality with extra layers, enhanced UI elements, and additional interactivity.

The duplicate `index.html` in the root is used to host the fully completed Part 2 version on GitHub Pages.

---

## Installation & Setup

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/leaflet-challenge.git
   cd leaflet-challenge
   ```

2. **Open the Project Locally**

   Since this project is built entirely with HTML, CSS, and JavaScript, you can open the files directly in your web browser. For development, it’s recommended to use a live server extension (for example, the Live Server extension in VS Code) to preview changes in real time.

3. **Optional: Set Up a Local HTTP Server**

   If you prefer using a simple HTTP server, you can run one using Python:
   ```bash
   # For Python 3.x
   python -m http.server 8000
   ```
   Then navigate to `http://localhost:8000/` in your browser.

---

## Technology Requirements

### Operating Systems

- **Supported OS:** Windows 10 or later, macOS, and Linux.
- **Minimum Hardware:**
  - CPU: Any modern processor
  - Memory: 2 GB RAM or higher
  - Disk Space: Minimal requirements as this is a front-end project

### Web Technologies

- **HTML5 & CSS3:** For structuring and styling the web pages.
- **JavaScript (ES6+):** For interactivity and logic.
- **Leaflet.js:** Core library for interactive mapping.
- **Additional Libraries:** (if any—include details on other frameworks/plugins used)

---

## Directory Structure & File Breakdown

Below is an overview of the project’s file structure along with brief explanations of each component:

```plaintext
leaflet-challenge/
│
├── part1/                       
│   ├── index.html              # Main HTML file for Part 1 with basic mapping features.
│   ├── css/                    
│   │   └── style.css           # Styles for Part 1.
│   ├── js/                     
│   │   └── main.js             # JavaScript logic for Part 1.
│   └── Images/                 # Image assets used in the project.
│       └── ...
│
├── part2/                       
│   ├── index.html              # Enhanced HTML file for Part 2 with extra functionality.
│   ├── css/                    
│   │   └── style.css           # Styles for Part 2 (may include additional customizations).
│   ├── js/                     
│   │   └── main.js             # JavaScript logic for Part 2 with extended features.
│   └── Images/                 # Image assets used in the project.
│       └── ...
│
├── index.html                  # Duplicate of Part 2's index.html for GitHub Pages deployment.
└── README.md                   # This README file.
```

### Explanation of File & Directory Connections

- **part1/**:  
  Contains the initial version of the project with core mapping functionality.
  
- **part2/**:  
  Includes the extended version of the project with optional enhancements. This version may feature additional layers, UI improvements, or interactivity.
  
- **index.html (root)**:  
  A duplicate of Part 2’s `index.html` is placed in the root directory to simplify hosting on GitHub Pages. This file is used as the landing page for the deployed site.
  
- **assets/**:  
  Stores static resources such as images and data files required for the map.
  
- **README.md**:  
  Provides an overview and instructions for the project.

---

## Deployment Details

- **GitHub Pages:**  
  The project is configured to deploy via GitHub Pages using the duplicate `index.html` (which is Part 2’s version) in the root directory. This ensures that visitors see the fully enhanced version of the project.

- **Hosting:**  
  Simply push your changes to the GitHub repository and enable GitHub Pages from the repository settings. The site will be accessible at [https://neelka96.github.io/leaflet-challenge/](https://neelka96.github.io/leaflet-challenge/).

---

## Usage

- **Local Development:**  
  Open the `part1/index.html` or `part2/index.html` (or the root `index.html` for the enhanced version) in your browser. Use a live server for real-time updates.

- **Viewing on GitHub Pages:**  
  Navigate to the GitHub Pages URL (e.g., [https://neelka96.github.io/leaflet-challenge/](https://neelka96.github.io/leaflet-challenge/)) to view the deployed site with the full functionality of Part 2.

- **Interactivity:**  
  Use the interactive controls provided by the Leaflet map to explore data layers and features. JavaScript files in the `js/` directories manage the map’s behavior and data interactions.

---

## Limitations

- **Browser Compatibility:**  
  While the project uses modern web standards, some legacy browsers may not fully support all features.
  
- **Data Integration:**  
  The project currently uses static data files (if any). Future iterations could integrate live data feeds or APIs for real-time updates.

- **Performance:**  
  Extensive data layers or high-resolution images may impact performance on lower-end devices.

- **Feature Completeness:**  
  Part 1 is a simplified version; Part 2 includes optional enhancements. Some users may prefer additional features which could be implemented in future updates.

---

## Credits & Citations

- **Key References & Documentation:**  
  - [Leaflet Documentation](https://leafletjs.com/)
  - [HTML5 & CSS3 Documentation](https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/HTML5)
  - [JavaScript (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)
  - README template and design guidance inspired by previous projects and ChatGPT assistance.
#EOF

# Matplotlib Challenge - Pymaceuticals
`Module 5`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
`By Neel Kumar Agarwal`  

## Table of Contents  
1. [Introduction](#introduction)  
2. [Challenge Overview](#challenge-overview)  
3. [Variables/Breakdowns](#variablesbreakdowns)  
    - [Relevant Variables](#relevant-variables)  
    - [Summary Breakdowns](#summary-breakdowns)  
3. [Setup and Usage](#setup-and-usage)  
    - [Prerequisites](#prerequisites)  
    - [Instructions](#instructions)  
    - [Limitations](#limitations)  
4. [Files and Directory Structure](#files-and-directory-structure)  
5. [Expected Results](#expected-results)  
6. [Final Analysis](#final-analysis)  



> [!NOTE]  
> Regarding citations...  
> No citations were needed for the completion of this challenge. "Starter Code" was  
> given by edX/2U to help guide students partaking in their program in initially  
> setting up the Jupyter Notebook. Other than that, all Source Code found in the  
> Notebook and in this README.md was written originally by @Neelka96  



## Introduction  
*Roleplay...*  
You've just joined Pymaceuticals, Inc., a new pharmaceutical company that  
specializes in anti-cancer medications. Recently, it began screening for potential  
treatments for squamous cell carcinoma (SCC), a commonly occurring form of skin cancer.  

As a senior data analyst at the company, you've been given access to the complete  
data from their most recent animal study. In this study, 249 mice who were identified  
with SCC tumors received treatment with a range of drug regimens. Over the course of  
45 days, tumor development was observed and measured. The purpose of this study was  
to compare the performance of Pymaceuticals’ drug of interest, Capomulin, against the  
other treatment regimens.  

The executive team has tasked you with generating all of the tables and figures needed  
for the technical report of the clinical study. They have also asked you for a top-level  
summary of the study results.  

[:arrow_up: Return to TOC](#table-of-contents)  



## Challenge Overview  
This assignment is broken down into the following tasks:  
- Prepare the data.  
- Generate summary statistics.  
- Create bar charts and pie charts.  
- Calculate quartiles, find outliers, and create a box plot.  
- Create a line plot and a scatter plot.  
- Calculate correlation and regression.  
- Submit final analysis.  

The Pymaceuticals .ipynb is a Python-interactive Jupyter notebook that takes raw data  
from a test study of mice with tumors, transforms it into usable information, and  
creates graphical presentations of different variables and their relationships. The  
exact process consists of extracting raw data, combining data together for usability  
and portability, cleaning our merged data, and then using it to create palpable  
summaries and charts detailing the effectiveness of different drug regiments.  

Pymaceuticals was developed and currently runs on a conda based local python  
environment. Python version used is `3.12.7` running with imported dependencies of  
`pathlib`, `random`, `pandas`, `matplotlib`, and `scipy`. The files Mouse_metadata.csv  
and Study_results.csv were imported and read into two Pandas DataFrame objects.  
The two frames were checked for holes in their data types (NaN). After merging the two  
DFs on their column axis by the unique ID of the mice involved in the study, another  
scan over the newly merged DF was done but this time to check for any duplicate  
information found under data identifiers.  

The identifying labels for each datapoint consists of a mouse ID and the timepoint the  
data was measured at (days since study started for mouse). Each mouse ID's timepoints  
must be unique to that ID. For any ID found with duplicate timepoints, all data  
associated with said ID must be removed to ensure data integrity.  

> [!NOTE]  
> In the case of a majority duplicates, a different course of action would be taken  
> to maintain a usable amount of datapoints, however only one mouse ID was found and  
> thusly removed from the set.  

After cleaning has been completed, the study's data has been combined with its  
corresponding mouse ID to give access to the mouse's data at every datapoint. Summary  
statistics of tumor by drug treatment is calculated, graphs visualizing the usage of  
each drug in the study and distribution of mice by gender are rendered, and then  
outliers are found quantitatively and qualitatively. Last of all, 


[:arrow_up: Return to TOC](#table-of-contents)  



## Variables/Breakdowns  
### Relevant Variables:  
```
- Mouse Data  
    - Mouse ID  
    - Drug Regimen  
    - Sex  
    - Weight (g)  
- Study Data  
    - Mouse ID  
    - Timepoint  
    - Tumor Volume (mm3)  

- Datapoint identifiers  
    - Mouse ID  
        -Timepoints  
```  
[:arrow_up: Return to TOC](#table-of-contents)  



### Summary Breakdowns:  
- Loading Station & Data Cleanup  
    - Setup for Imports, CSVs, and DataFrames  
    - Locating Duplicate Mice by ID and Timepoint  
    - Duplicate Mice Data
    - Cleaned DataFrame
- Summary Statistics  
- Bar and Pie Charts  
- Quartiles, Outliers and Boxplots  
- Line and Scatter Plots  
- Correlation and Regression  

[:arrow_up: Return to TOC](#table-of-contents)  



## Setup and Usage  
### Prerequisites  
- Python 3.x  
- Standard libraries: `pathlib`, `random` (included with Python)  
- Non-standard library: `pandas`, `matplotlib`, `scipy`  
- IDE that supports Jupyter Notebooks with Python  

[:arrow_up: Return to TOC](#table-of-contents)  



### Instructions  
1. Clone this repository.  
2. Ensure IDE is up to date and running.  
3. Ensure the two input CSV files are in the `data` sub-folder.  
4. Open `main.ipynb` in your IDE and run all cells.  
5. If the necessary dependencies aren't found install using the following methods:  
    - For *pip*  
        ```  
        pip install pathlib  
        pip install random  
        pip install pandas  
        pip install matplotlib  
        pip install scipy  
        ```  
    - For *anaconda*  
        ```  
        conda install pandas  
        conda install matplotlib  
        conda install scipy  
        ```  
> [!WARNING]  
> Please note that neither *pathlib* nor *random* was installed using conda.  
> *pathlib* and *random* are base modules almost always included with Python  
> installations. It is recommended to use `pip install` for these two modules.  
6. **Results will print throughout the Jupyter Interactive Notebook**  

[:arrow_up: Return to TOC](#table-of-contents)  



### Limitations  
- [ ] Graphed results are qualitative not quantitative    
- [ ] Time frame for DataFrames is limited up to 45 days instead of an open dynamic range  

[:arrow_up: Return to TOC](#table-of-contents)  



## Files and Directory Structure  
```  
matplotlib-challenge/
|
|— Pymaceuticals/
|   |— data/
|   |   — Mouse_metadata.csv
|   |   - Study_results.csv
|   |— main.ipynb  
|   |- README.md
```  
This structure ensures all inputs are organized within their respective folders.  

[:arrow_up: Return to TOC](#table-of-contents)  



## Expected Results  
Results from this code are solely for viewing in the Jupyter Notebook. As many  
dependencies have already been loaded into the code it is needless to add another  
just to print the graphs outside of the main.ipynb file. Please see the `Tip` for  
a Jupyter Notebook traversal suggestion.

> [!TIP]  
> Collapsing of various regions or use of `OUTLINE` in VSCode can  
> speed up exploration of the Notebook.  

The following are examples of the expected results from the matplotlib code running  
in the Jupyter Notebook:

1. This Bar Chart shows the number of timepoints contained in ecah drug regimen (aka number of total doses).  
![Bar Chart for the number of Timepoints for each drug regimen](docs/figures/Obs_perDrug.png)  
  
2. This pie chart depicts the percent of whole gender distribution of the mice in this experiment.  
![Pie Chart for the gender distribution of mice](docs/figures/mouse_perGender.png)  
  
3. This boxplot displays the inner and outer quartile ranges of the top 4 performing drug regimens in the study.  
![Boxplot chart for quartiles and outliers of the top 4 performing drug regimens](docs/figures/outliers_topDrugs.png)  
  
4. This line graph is randomly generated each time the code is run, as the challenge called for 'a mouse', and not a specific or consistent one throughout.  
The line graph is for the volume of its tumor over its timepoints in the study.  
![Random line graph for the tumor volume of a random mouse over its timepoints (Not indicative of what will actually be produced because mouse selection is randomized)](docs/figures/random_mouse_tumor.png)  
  
5. This scatter plot is for the purposes of displaying a single mouse's average tumor size vs the weight of that same mouse.  
![Scatter plot chart for the average volume of a tumor vs the weight of the mouse that had that average tumor size](docs/figures/weight_avgTumor.png)  
  
6. The chart uses the previous scatter plot but layers a linear regression drawn line on top of it, along with an annotation for the equation of that line.  
![Linear regression and line equation annotations added on-top of previous scatter plot](docs/figures/weight_avgTumor_regress.png)  

[:arrow_up: Return to TOC](#table-of-contents)  



## Final Analysis  
While the study was held for the purpose figuring out which drug regimen was  
the most successful in treating squamous cell carcinoma (SCC), it also shed a  
light on how mice in general react to anti-cancer drugs and solutions. Distributions  
within different groupings are largely similar enough to compare and of a big  
enough size to analyze in the first place.  

Finding the highest performing treatments signals a few things:  
    1. The lowest tumor size mean of a drug regimen  
    2. Smaller median value for tumor size by drug  
    3. Lower standard deviation in tumor size  
    4. Final tumor sizes are the smallest  

The boxplot could be easily expanded to include all drug regimens, but based off  
the evidence accumulated from the summaries and using the following code a cleaner  
picture is painted as to how well the drugs perform.  
```
INPUT:

mean = clean_df_summary.sort_values('Mean Tumor Volume').iloc[:5, 0].index
median = clean_df_summary.sort_values('Median Tumor Volume').iloc[:5, 1].index
variance = clean_df_summary.sort_values('Tumor Volume Variance').iloc[:5, 2].index
std_dev = clean_df_summary.sort_values('Tumor Volume Std. Dev.').iloc[:5, 3].index
std_err = clean_df_summary.sort_values('Tumor Volume Std. Err.').iloc[:5, 4].index
rank = 1
print('          Mean     Median    Variance    Std Dev   Std Err')
print('-'*65)
for a,b,c,d,e in zip(mean, median, variance, std_dev, std_err):
    print(f'Rank {rank}: {a}, {b}, {c}, {d}, {e}')
    rank += 1
```
```
OUTPUT:

          Mean     Median    Variance    Std Dev    Std Err
-----------------------------------------------------------------
Rank 1: Ramicane, Ramicane, Ramicane, Ramicane, Ramicane
Rank 2: Capomulin, Capomulin, Capomulin, Capomulin, Capomulin
Rank 3: Propriva, Propriva, Ceftamin, Ceftamin, Ceftamin
Rank 4: Ceftamin, Ceftamin, Infubinol, Infubinol, Infubinol
Rank 5: Infubinol, Zoniferol, Propriva, Propriva, Zoniferol
```
The code separates the different columns and isolates the top five drugs  
for the mean, median, variance, standard deviation, and standard error.  
Based off of the summaries the top drugs are Ramicane, Capomulin, Propriva,  
Ceftamin, and Infubinol. However when viewed in the boxplot form, it has a wider  
range and margin of error than it's main competitors Ceftamin and Infubinol, and  
it's among the drugs with the least number of observations. Therefore, it makes  
sense that the boxplot requested was for the augmented top four drugs: Ramicane,  
Capomulin, Ceftamin, and Infubinol.  

As part of the analysis, a deeper look at the individual mice who received  
Capomulin was requested because it's a top contender. In order to tackle this,  
and because no specific mouse was requested, a random generator for any mouse  
that was treated with Capomulin was created so that during each runtime a random  
mouse was selected and graphed according to its timepoints and tumor volume.  
The randomly graphed mice that were treated with Capomulin almost always appear  
to have a downwards correlation between timepoints and tumor volume. However,  
this random generator could be further modulated and switched to a different drug  
regimen, and when used in examination of random mice that were treated with  
Ramicane the mice actually seem to outperform the mice that were treated with  
Capomulin. Further analysis could most certainly be used, especially a longer-term  
study or maintaining more detailed data sets of the mice in the study along with  
outcomes or side-affects of different drug regimens. Without said information,  
it is suffice to say with this data set that Ramicane and Capomulin are the best  
performing drug regimens for mice to take in countering squamous cell carcinoma  
(SCC).

[:arrow_up: Return to TOC](#table-of-contents)  
#EOF

# NoSQL Challenge – Eat Safe, Love
`Module 12`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
By: **Neel Kumar Agarwal**

## Table of Contents
1. [Introduction](#introduction)  
2. [Challenge Overview](#challenge-overview)  
3. [Deliverables](#deliverables)  
4. [Setup and Usage](#setup-and-usage)  
   - [Prerequisites](#prerequisites)  
   - [Instructions](#instructions)  
   - [Limitations](#limitations)  
5. [Files and Directory Structure](#files-and-directory-structure)  
6. [Expected Results](#expected-results)  
7. [Analysis & Explanation](#analysis--explanation)  
   - [Database Setup (Part 1)](#database-setup-part-1)  
   - [Update the Database (Part 2)](#update-the-database-part-2)  
   - [Exploratory Analysis (Part 3)](#exploratory-analysis-part-3)  
8. [Citations / References](#citations--references)

---

## Introduction
In this challenge, we explore and analyze a dataset from the UK Food Standards Agency using **MongoDB** (a NoSQL database). We:

1. **Set up** a MongoDB database named `uk_food`.
2. **Import** a large JSON dataset containing establishment information.
3. **Query** and **update** the database with Python using `pymongo`.
4. **Perform Exploratory Analysis** on the data to answer questions such as:
    - Which establishments have a hygiene score of 20?
    - Which are in London with a certain rating?
    - How many have a hygiene score of 0 in each Local Authority area?

The final product is an operational local MongoDB with an updated dataset plus code-based queries and analysis in Jupyter Notebooks.

---

## Challenge Overview  
1. **Part 1**: **Database and Jupyter Notebook Set Up**  
    - Import the `establishments.json` file into MongoDB.  
    - Verify the database creation and data insertion.  

2. **Part 2**: **Update the Database**  
    - Insert a new restaurant ("Penang Flavours") into the collection.  
    - Adjust `BusinessTypeID` for the new entry.  
    - Remove undesired documents (e.g., those with `LocalAuthorityName` = "Dover").  
    - Clean up data types for `latitude`, `longitude`, and `RatingValue`.  

3. **Part 3**: **Exploratory Analysis**  
    - Query for hygiene score = 20.  
    - Query for certain local authorities with rating >= 4.  
    - Find top establishments with rating = 5 near the newly inserted restaurant.  
    - Aggregate documents by `LocalAuthorityName` for those with a hygiene score = 0.  

By the end, we have a **NoSQL** dataset loaded into MongoDB with relevant queries and manipulations performed.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Deliverables
1. **NoSQL_setup.ipynb**  
    - Executes tasks to connect to MongoDB, create/insert documents, remove specific entries, and adjust data types. Prints results at various points to the cell outputs in Jupyter Notebook.  

2. **NoSQL_analysis.ipynb**  
    - Performs exploratory queries and aggregations.  
    - Prints results to the cell outputs in Jupyter Notebook.  

3. **README.md** (this file)  
    - Summarizes the project, usage instructions, and major findings.  

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Setup and Usage
### Prerequisites
- **Python 3.x**  
- **MongoDB** server installed and running on localhost `port = 27017`.  
- **pymongo** library for Python (`pip install pymongo`).  
- **pandas** Library for Python Data Manipulation (`pip install pandas`).  
- A Jupyter Notebook or equivalent environment to run / view code output.  
- The **establishments.json** file provided by the challenge.  

### Instructions
1. **Install Dependencies**:  
    ```bash  
    pip install pymongo
    ```  

2. **Ensure MongoDB** is installed locally and running 
    - For Linux: `sudo service mongod start`  
    - For Mac: `brew services start mongodb-community@8.0`  
    - For Windows: *No operation should be necessary*
    - Or using MongoDB Compass App  

3. **Clone** this repository via HTTPS/SSH (from [GitHub Link](https://www.github.com/Neelka96/nosql-challenge)).  

4. **Import the data** (as per instructions in the assignment):  
    ```bash
    # Navigate to the Repo Clone Directory
    cd YOUR/PATH/TO/REPO/HERE/nosql-challenge

    # From the directory where your JSON file is located:
    mongoimport --type json -d uk_food -c establishments --drop --jsonArray Resources/establishments.json
    ```  

5. **Run all cells in `NoSQL_setup.ipynb`** to:  
    - Connect to MongoDB.  
    - Verify the database and collection.  
    - Insert a new restaurant.  
    - Perform data cleaning / type casting.  
    - Shows results of validating CRUD throughout Notebook.  

6. **Run all cell in `NoSQL_analysis.ipynb`** for the exploratory queries and aggregation tasks:
    - Identify establishments with hygiene score = 20.  
    - Compare rating values, etc.  
    - Shows the results of exploration throughout Jupyter environment.  

### Limitations
- **Local environment**: The code expects a local MongoDB instance on port 27017. For other setups, update your `MongoClient` connection string.  
- **Large dataset**: The JSON file may contain thousands of documents, so queries or updates can take noticeable time depending on your hardware.  
- **Static dataset**: This challenge uses a static sample of the UK Food dataset (not automatically updated).  

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Directory Structure
```
NoSQL-challenge/
├── Resources/
│   └── establishments.json
│
├──.gitignore
├── NoSQL_analysis.ipynb
├── NoSQL_setup.ipynb
└── README.md
```

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Expected Results
After running **NoSQL_setup.py**:
- A new document for **Penang Flavours** is inserted with the correct `BusinessTypeID`.  
- All entries with `LocalAuthorityName` = "Dover" are removed.  
- Coordinates and `RatingValue` are cast to numeric types.  

After running **NoSQL_analysis.py**:
- You’ll see the number of establishments with a hygiene score = 20, and a sample document printed.  
- You’ll see a list of establishments in “London” with rating >= 4.  
- The top 5 with rating=5 near “Penang Flavours” will be displayed, sorted by hygiene.  
- A final aggregation showing how many establishments in each `LocalAuthorityName` have a hygiene score = 0.  

Use a Jupyter Notebook or any other environment that can load and run these `.py` scripts to view the logs and results.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Analysis & Explanation

### Database Setup (Part 1)
1. **Importing JSON**: We drop the existing `establishments` collection and load the data from `establishments.json` into `uk_food.establishments`.  
2. **Validation**: We check if the database and collection exist, verifying document counts.

### Update the Database (Part 2)
1. **Insert “Penang Flavours”**: A dictionary object is inserted into the `establishments` collection.  
2. **Adjust Field**: We retrieve the correct `BusinessTypeID` for “Restaurant/Cafe/Canteen” and apply that to the new record.  
3. **Remove Dover**: We remove all documents with `LocalAuthorityName` = “Dover.”  
4. **Cleaning**: We cast `latitude`/`longitude` to `float` (double in Mongo) and convert `RatingValue` to integer (null for certain non-integer values).

### Exploratory Analysis (Part 3)
1. **Hygiene Score == 20**: We locate all documents where `scores.Hygiene` = 20.  
2. **Greater or Equal to 4**: We locate establishments with `LocalAuthorityName` containing “London” and `RatingValue >= 4`.  
3. **Rating=5, sorted by Hygiene** near "Penang Flavours": We find top five within ±0.01 degrees lat/long.  
4. **Aggregate by Hygiene=0**: We group by `LocalAuthorityName` and count how many have `scores.Hygiene`=0, sorted descending.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Citations / References  
- **EdX/2U**: Provided the dataset and instructions for the “NoSQL Challenge.”  
- **README.md**: Created using OpenAI's [ChatGPT LLM](https://www.chatgpt.com), trained using prior READMEs from project owner and sole contributor's repository [Neel Agarwal (Neelka96)](https://www.github.com/Neelka96), the two deliverables, and the provided rubric given by edX/2U  
- **MongoDB Documentation**: [https://www.mongodb.com/docs/manual/](https://www.mongodb.com/docs/manual/)  
- **pymongo Documentation**: [https://pypi.org/project/pymongo/](https://pypi.org/project/pymongo/)  

[:arrow_up: Return to TOC](#table-of-contents)  

---
#EOF

# Pandas Challenge - PyCitySchools  
`Module 4`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
`By Neel Kumar Agarwal`  

## Table of Contents  
1. [Introduction](#introduction)  
2. [Challenge Overview](#challenge-overview)  
3. [Variables/Breakdowns](#variablesbreakdowns)  
    - [Relevant Variables](#relevant-variables)  
    - [Summary Breakdowns](#summary-breakdowns)  
4. [Setup and Usage](#setup-and-usage)  
    - [Prerequisites](#prerequisites)  
    - [Instructions](#instructions)  
    - [Limitations](#limitations)  
5. [Files and Directory Structure](#files-and-directory-structure)  
6. [Expected Results](#expected-results)  
7. [Final Analysis](#final-analysis)  
    - [Type & Size of School](#type--size-of-school)
    - [Reading & Math Scores](#reading--math-scores)


## Introduction  
You are the new Chief Data Scientist for your city's school district. In this  
capacity, you'll be helping the school board and mayor make strategic decisions  
regarding future school budgets and priorities.  

As a first task, you've been asked to analyze the district-wide standardized test  
results. You'll be given access to every student's math and reading scores, as  
well as various information on the schools they attend. Your task is to aggregate  
the data to showcase obvious trends in school performance.  

## Challenge Overview  
The PyCitySchools .ipynb is a Python-interactive Jupyter notebook that aims to  
find global and local trends of district, school, and student standardized testing  
scores. My job was to take data presented in cleaned CSV file format and extract  
relevant information from it to produce snapshots of different variants and their  
relationally aggregated data.  

PyCitySchools runs using a conda based local python environment. Python version  
used is `3.12.7` running with dependencies of Pandas and pathlib. Two clean CSV  
files, one containing school data and the other student data, were imported and  
read into 2 Pandas DataFrame objects. Minor data validation was performed but not  
needed. The two DataFrames (DFs) were then merged on their shared factor which was  
the school name they contained. A complete DF of both CSV files was then usable  
after tidying. This complete DF was used in the entirety of the Jupyter Notebook to  
create new DFs that held concise summary information of each comparison. After each  
comparison is made an image of it is saved to `PyCitySchools/snapshots/<image-name>`.  

The task at hand is to compare how successful students and schools are, dependently  
and independently, at standardized tests for reading and mathematics. Instances in  
which both tests are passed are also found taken into account.  

> [!NOTE]
> No citations are needed for this project save for the code, data, and assigned  
> conditions given by edX (2U) given for use of learning in this bootcamp.  

## Variables/Breakdowns  
### Relevant Variables:  
```
- School_Name  
    - Type (District/Charter)
    - Size
    - Budget
- Student_Name  
    - Grade (9-12)
    - Reading Score
    - Math Score
```
### Summary Breakdowns:  
```
|-*-| District Summary
|-*-| School Summary
|-*-| Highest-Performing School (by % Overall Passing)
|-*-| Bottom-Performing Schools (by % Overall Passing)
|-*-| Math Scores by Grade
|-*-| Reading Scores by Grade
|-*-| Scores by School Spending
|-*-| Scores by School Size
|-*-| Scores by School Type
```

## Setup and Usage  
### Prerequisites  
- Python 3.x  
- Standard libraries: `pandas` and `pathlib` (included with Python)  
- Non-standard library: `dataframe_image`  
- IDE that supports Jupyter Notebooks with Python  

### Instructions  
1. Clone this repository.  
2. Ensure IDE is up to date and running.  
3. Please run the following code in your IDE if the dependencies aren't found:  
    ```
    pip install pandas
    pip install pathlib
    pip install dataframe-image
    ```
4. Ensure the input CSV files are in the `Resources` folder.  
5. Open `main.ipynb` in your IDE and run all cells.  
6. Results will print throughout the Jupyter Interactive Notebook  
7. Results are then exported in .png format in new `snapshots` directory  
> [!TIP]  
> Collapsing of various regions or use of `OUTLINE` in VSCode can  
> speed up exploration of the Notebook.  
> If examining using .png images, please refer to the image file  
> name to find the comparison being used to calculate the DataFrame.  

### Limitations  
- [ ] Results are qualitative not quantitative  
- [ ] Exported images lack complete titling for easy comprehension in any space  
- [ ] Time frame for DataFrame is a static moment instead of a dynamic range  


## Files and Directory Structure  
```  
pandas-challenge/
|
|— PyCitySchools/
|   |— Resources/
|   |   — schools_complete.csv
|   |   - students_complete.csv
|   |— main.ipynb
```  
This structure ensures all inputs are organized within their respective folders.  
Outputs will be created without additional directory structuring  


## Expected Results  
1. Updated Jupyter Notebook DataFrames and printed DataFrames
2. New directory structure for `pandas-challenge/`:
```
pandas-challenge/
|
|— PyCitySchools/
|   |— Resources/
|   |   — schools_complete.csv
|   |   - students_complete.csv
|   |- snapshots/
|   |   - bottomPerforming_byOverallPassing.png
|   |   - district_snapshot.png
|   |   - mathScores_byGrade.png
|   |   - perSchool_summary.png
|   |   - readScores_byGrade.png
|   |   - scores_bySchoolType.png
|   |   - scores_bySize.png
|   |   - scores_bySpending.png
|   |   - topPerforming_byOverallPassing.png
|   |— main.ipynb (updated)
```
New directory paths indicate proper running and exporting of DataFrames.

---

## Final Analysis:    
### Type & Size of School:  
- **<ins>Intro:</ins>**  
    It's immediately apparent from the `Highest-Performing Schools (% Overall Passing)`  
    and `Lowest-Performing Schools (% Overall Passing)` or even better the  
    `Scores by School Type` that there is a major difference between charter  
    and district type schools based off of their `% Overall Passing`.  

- **<ins>Observable Quantity:</ins>**  
    In all scenarios, charter type schools had better testing  
    scores than district type schools by a drastic margin. On average the  
    percent difference of students passing both reading and math exams from  
    charter and district type schools was 36.76%.  

- **<ins>Important Relationships:</ins>**  
    Charter and district school observations are also  
    recognizable from their budgets or sizing without the school type when  
    binned and categorized. This is because the budgets for each school is  
    proportional to the size of the school and the way their resources are allocated.  
    Generally, charter schools may spend less per student while districts spend more  
    per student, but the budget is not strictly indicative of how well students did.  
    It is actually more representative of the schools themselves and their need for  
    more resources as a larger school that serves a wider demographic.  

- **<ins>Main Factors:</ins>**  
    All charter schools, except one, are classified as `Small (<1000)`  
    or `Medium (1000-2000)` sized schools, while every single district school is  
    `Large (2000-5000)`. Ignoring the school type completely, the data in  
    `Scores by School Size` shows that the rates of success of large size schools  
    are almost identical the rate of success of district type schools, and the  
    rates of success of medium and small sized schools are both approximately  
    90% (near to the high level of success seen in charter schools). This data by  
    itself can suggest that, regardless of the budget or type of the school,  
    schools that have more students have difficulty maintaining high test scores  
    with all students.  

- **<ins>Implied Story:</ins>**  
    Taking the type of school into account again, the aforementioned  
    difficulty in raising/maintaining test scores could be for a plethora of reasons  
    some of which are outside the control of the school. For example, which  
    students are assigned to which school zones, whether some of their budget per  
    students has to be allocated to additional social services that aren't generally  
    required of charter schools, and how many students are assigned to each classroom.  
    The way the size of a school changes the performance of each individual student isn't  
    always important but it seems that once it reaches a certain number the success  
    rate begins to drop, especially in mathematics.  

- **<ins>Summary Story:</ins>**  
    The larger the school is the higher the chance is of a student not  
    receiving adequate resources to help achieve passing test scores. Additionally, within  
    this School District, charter and district type schools are almost exclusively of a  
    smaller and larger size respectively.  


### Reading & Math Scores:  
- **<ins>Intro:</ins>**  
    While there isn't an obvious correlation between them, the snapshots of  
    `Reading Scores by Grade` and `Math Scores by Grade` do shed light on how difficult  
    students find the tests depending on their grade. These snapshots are helpful but a  
    little limited because it only shows how difficult random samples of students did in  
    each grade, and not how difficult a student found the tests in each of the grades.  

- **<ins>Observable Quantity:</ins>**  
    While reading and math scores on average don't deviate too  
    strongly from each other, math scores do vary the most out of the two, especially in  
    relation to other variables in question. In terms of impact, math scores make the  
    largest ones for the `% Overall Passing` in every consecutive DataFrame. The quantitative  
    data shows:
    ```
    Read Scores by Grade Numerical Summary (Averages for 9th-12th):
        Mean = 82.53326469678984
        Max = 84.36234496688002
        Min = 80.49302138528849
        Diff = (3.86932358159153)
        Std Dev = 1.5707706608878393

    Math Scores by Grade Numerical Summary (Averages for 9th-12th):
        Mean = 80.4325622269647
        Max = 84.28450314388449
        Min = 76.22184046310876
        Diff = (8.06266268077573)
        Std Dev = 3.4041960121066586
    ```
    These values provide truth to the prior and following statements in multiple ways.  
    Primarily, the standard deviation of the math scores by grade is more than twice that  
    of the reading scores by grade, which indicates that the spread of the math scores is  
    more than twice that of the reading score by grade. Lastly, the difference between  
    maximums and minimums follows this trend and shows that the scores at the tail ends of  
    the spread are even further apart with the minimum and maximum of the math scores being  
    lower than that of their respective reading scores.  

- **<ins>Important Relationships:</ins>**  
    When the metric of pass/fail at a score of `70.00` is  
    brought into the picture, the difference between a score of `75` and `80` is weighted  
    heavier than that of `85` and `95`. Math scores in particular reflect this property  
    whether it's in regards to math scores at different schools or reading scores. What's  
    also interesting about this district is the lack of any schools that show improvement in  
    their scores from one grade to the next. Students in grades 9th through 12th consistently  
    score the same throughout their school, meaning the scores are completely independent of  
    grades, but even more depedent on the schools.  

- **<ins>Main Factors:</ins>**  
    It's stated within [Type & Size of School](#type--size-of-school) that the type and size were  
    deterministic of each other (intertwined as variables), and size is what essentially  
    impacted the `% Overall Passing` values the most as it also impacts all the other factors  
    that are contributing to student test score. However, with this new lensing of the data,  
    we can also see that there is a high consistency of scores throughout each school for each  
    subject, independent of the grades, `9th, 10th, 11th, 12th`. A lack of change is not  
    inherently a negative feature, but from this perspective the scores that are nearing the  
    pass/fail limit must first be improved before overall maintenance can be considered.  
    Consistency, however, within this limited time frame of only one entry for one student,  
    is also an indicator of stabilization. Should more data be brought into the fold, a measurement  
    of how much a student is improving or declining could be added and used to uncover more information.  

- **<ins>Implied Story:</ins>**  
    The story that becomes clearer is of how schools are unable to help the students that are  
    struggling more in math even as they progress throughout the  school. Schools that have the  
    lowest math scores are incredibly indicative of schools with the lowest `% Overall Passing`.  
    In general, math tends to have a less comprehensive understanding by all demographics of people  
    and for all extensive purposes can be considered harder at times, and is most likely why schools  
    that struggle with resource allocation for larger class sizes would struggle even more with teaching  
    a difficult subject to a wider demographic of people.  

- **<ins>Summary Story:</ins>**  
    Math is already a difficult subject, but the data suggests that schools struggle with teaching students  
    regardless of resource allocation, however those with less resources per student will struggle even more  
    and produce worse test results overall. It's very likely that a worse a student is doing in math, the less  
    time they'll be able to produce quality results for reading as well.
#EOF

# Python API Challenge - Vacation_WeatherPy
`Module 6`  
`(edX/2U & UT) Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
`By Neel Kumar Agarwal`  

## Table of Contents  
1. [Introduction](#introduction)  
    - [WeatherPy](#weatherpy)  
    - [VacationPy](#vacationpy)  
2. [Setup and Usage](#setup-and-usage)  
    - [Prerequisites](#prerequisites)  
    - [Instructions](#instructions)  
    - [User Defined Module - gcs_fx](#user-defined-module---gcs_fx)  
3. [Files and Directory Structure](#files-and-directory-structure)  
4. [Expected Results](#expected-results)  



## Introduction  
This challenge is broken into two assignments: WeatherPy and VacationPy.  


### WeatherPy:  
Data's true power is its ability to definitively answer questions. So, let's take what  
you've learned about Python requests, APIs, and JSON traversals to answer a fundamental  
question: "What is the weather like as we approach the equator?"  

Now, we know what you may be thinking: “That's obvious. It gets hotter.” But, if pressed  
for more information, how would you prove that?  

So, in this deliverable we'll create a Python script to visualize the weather of over 500  
cities of varying distances from the equator. We'll use the citipy Python libraryLinks to  
an external site., the OpenWeatherMap APILinks to an external site., and our problem-solving  
skills to create a representative model of weather across cities.  


### VacationPy:  
Well, if we can do that, then surely we can work our magic to plan a perfect destination  
vacation based off of climate. Using the power of a different API and method for calling  
that API (as per usual), we can even graph the data in the form of a map.  

In this deliverable, we'll use your weather data skills to plan future vacations. Also,  
we'll use Jupyter notebooks, the geoViews Python library, and the Geoapify API. Our main  
tasks will be to use the Geoapify API and the geoViews Python library and to employ our  
Python skills to create map visualizations.  

[:arrow_up: Return to TOC](#table-of-contents)  


## Setup and Usage  
### Prerequisites  
- Python 3.x  
- Standard libraries: `pathlib`, `time`, `requests` (included with Python)  
- Non-standard library: `pandas`, `numpy`, `matplotlib`, `scipy`, and `citipy`  
- IDE that supports Jupyter Notebooks with Python  

[:arrow_up: Return to TOC](#table-of-contents)  



### Instructions  
1. Clone this repository.  
2. Ensure IDE is up to date and running.   
3. Open `WeatherPy.ipynb` in your IDE and run all cells.  
4. If the necessary dependencies aren't found, install using the following methods:  
    - For *pip*  
        `pip install <missing_library>`  
    - For *anaconda*  
        `conda install <missing_library>`  
> [!WARNING]  
> Please note that neither *pathlib*, *time*, nor *requests* was installed using conda.  
> *pathlib*, *time*, and *requests* are base modules almost always included with Python  
> installations. It is recommended to use `pip install` for these two modules to avoid  
> path dependencies.  
5. IDE/Terminal (depending on how code is being executed) will prompt user for 'confirm'
6. **Results will print throughout the Jupyter Interactive Notebook**  

[:arrow_up: Return to TOC](#table-of-contents)  



### User Defined Module - gcs_fx
A small python file has been created alongside the Jupyter Notebook `WeatherPy.ipynb` for the  
purpose of creating a separation between the heavy-lifting of graphing methods required by the  
assigned challenge, as a microcosm version of a 'back-end'. Said file is named GCS_Fx.py and  
runs under the alias gcs in the notebook. Modules `numpy`, `matplotlib.pyplot`, `time`, `scipy`,  
`citipy`, and `requests` have all been transferred to GCS_Fx to enhance readability of WeatherPy  
and allow for focus of modifying data as DataFrames via `pandas` and duplicating graphs with  
reoccurring corrections of labels and titles. The automatic generation of geographic coordinate  
pairs provided in the source code by edX and 2U has also been moved to the backend and given  
arguments that allow for slight customization using `citipy` and `numpy`, as well as the API calls  
used to retrieve the graphed data. The API calls are set by default for the purpose of this  
challenge but are moved to the backend and given arguments to change the queries.  

Top-Level graphing methods group multiple matplotlib.pyplot functions together so only one line  
of code must be deployed to create the graphs and display the data/results. Top-level code has  
been given default arguments for preferred outputs but allows for changes based on slightly  
different needs of different data outputs. In doing so, sensitive `pyplot` methods are being  
protected from errors while still allowing for custom outputs. almost all of the methods used  
in the Notebook could be modulated, graphical output is the least subject to changes, whereas  
if the DataFrames used are changed the output will still be correct if 'back-end' directories  
for label constructors are updated accordingly. Additional `update` methods could be added as  
well. Please be advised, this was done for practice as well as improving the usefulness of the  
notebook and cleaning up the code in general.  

The following shorthand list was used initially to help frame the reasons why this path was  
taken as opposed to re-coding it each time:
- Readability
- Easy customization
- Replication
- Protection of sensitive methods
- Separation of dependencies into another Python file:
    - Cleans/protects namespace
    - Creates specific roles --> Which code is more variable?
        - DataFrames are viable to changes
        - Pyplot objects are subject to specific designations/formatting
            - Methods can always be updated for more label corrections and formatting req,
            --> Simply add more names into the directory
        - API call can be considered non-variable but arguments for changing queries are provided

[:arrow_up: Return to TOC](#table-of-contents)  


## Files and Directory Structure  
```  
python-api-challenge/
|
|-- Vacation_WeatherPy
|   |—- output_data/
|   |   |—- cities.csv
|   |-- gcs_fx.py
|   |-- VacationPy.ipynb
|   |-- WeatherPy.ipynb
|-- .gitignore
|-- README.md
```  
This structure ensures all inputs are organized within their respective folders.  
Outputs will be created without additional directory structuring  

[:arrow_up: Return to TOC](#table-of-contents)  
#EOF

# Python Challenge
`Module 3`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`    

## Table of Contents
1. [Introduction](#introduction)
2. [Challenges Overview](#challenges-overview)
   - [PyBank](#pybank)
   - [PyPoll](#pypoll)
3. [Setup and Usage](#setup-and-usage)
4. [Expected Results](#expected-results)
5. [Files and Directory Structure](#files-and-directory-structure)
6. [Citations](#citations)

## Introduction
This repository contains Python scripts for two data analysis challenges!  
Each script reads a CSV file, processes the data, formats it, and outputs  
a summary to the terminal and a text file saved to a designated path.  
### PyBank: 
Uses the CSV of monthly budget data from a bank to perform financial data  
analysis and provide a readable summary of the number of months, net profit,  
average of a change list, maximum(s), and minimums(s). [Link to PyBank main!](/PyBank/main.py)
### PyPoll: 
Uses the CSV of unique voter data from an election to perform election data  
analysis and provide a readable summary of the total voters, the number of  
votes for each candidate along with that percent of the total vote, and the  
winner(s) of the election based off popular vote. [Link to PyPoll main!](/PyPoll/main.py)
> [!WARNING]
> The CSV files in use were provided as learning material by EdX/UT  
> for use in their data viz course. Please be aware that data is most  
> likely auto-generated and not representative of any banks or elections.  

## Challenges Overview
Both challenges were originally completed using a global running of the script, but has  
since been modified for use as module/import, and to clarify algorithm process in main(). 

### PyBank
The PyBank script analyzes financial records to calculate:
- Total months in the dataset
- Net total profit/loss
- Average change in profit/loss between months
- Greatest increase and decrease in profits
     - Validates multiple max increase/decrease values

### PyPoll
The PyPoll script analyzes election data to determine:
- Total votes cast
- Votes and percentage of votes for each candidate
- Winner(s) based on popular vote
     - Validates multiple winners and outputs warning

## Setup and Usage
### Prerequisites
- Python 3.x
- Standard libraries: `csv` and `os` (included with Python)

### Limitations
- Doesn't verify unique voter ballot IDs
- Algorithms are basic/brute-force for the most part, so the complexity will raise linearly
- Readability: More Pythonic methods available in pandas (using basic methods per class instruction)

### Instructions
1. Clone this repository.
2. Ensure the input CSV files are in the `Resources` folder.
3. Run the scripts from the command line:
   - PyBank: `python PyBank/main.py`
   - PyPoll: `python PyPoll/main.py`
4. Results will appear in the terminal and be saved to text files in the `analysis` folder.

## Expected Results
### PyBank Output
```
Financial Analysis
-------------------------
Total Months: 86
Total: $$22564198
Average Change: $-8311.11
Greatest Increase in Profits: Aug-16 ($1862002)
Greatest Decrease in Profits: Feb-14 ($-1825558)
```

### PyPoll Output
```
Election Results
-------------------------
Total Votes: 369711
-------------------------
Charles Casper Stockham: 23.049% (85213)
Diana DeGette: 73.812% (272892)
Raymon Anthony Doane: 3.139% (11606)
-------------------------
Winner: Diana DeGette
-------------------------
```

## Files and Directory Structure
```
PyBank-and-PyPoll/
|— PyBank/
|   |— Resources/
|   |   — budget_data.csv
|   |— analysis/
|   |   — budget_analysis.txt
|   — main.py
|— PyPoll/
|   |— Resources/
|   |   — election_data.csv
|   |— analysis/
|   |   — election_analysis.txt
|   — main.py
```
This structure ensures all inputs and outputs are organized within their respective folders.

## Citations
### StackOverflow Module Help
In the process of re-learning and practicing function calls and passing return values to the next  
function, I stumbled upon an interesting protocol that most .py files follow to prevent unintentional  
execution of imported files, and is a useful habit to have in terms of overall syntax.  
> @Mr Fooz. "What does if __name__ == "__main__": do?" Stack Overflow, January 7, 2009. https://stackoverflow.com/questions/419163/what-does-if-name-main-do  
### OpenAI ChatGPT
ChatGPT was utilized in this assignment for help generating the framework for this README.md.  
OpenAI's generativeAI was fed the rubric for this assignment to assist it in creating an accurate  
structure. README.md at this time differs largely in resemblence to original generated response.  
> OpenAI. (2024). ChatGPT (ChatGPT-4o) [Large language model]. https://chatgpt.com  


#EOF

# SQL Challenge - EmployeeSQL  
`Module 9`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
`By Neel Kumar Agarwal`  

## Table of Contents  
1. [Introduction](#introduction)  
2. [Challenge Overview](#challenge-overview)  
3. [Entity Relationship Diagram](#entity-relationship-diagram-erd)  
4. [Summary Breakdowns](#summary-breakdowns)  
5. [Setup and Usage](#setup-and-usage)  
    - [Prerequisites & Instructions](#prerequisites--instructions)  
    - [Directory Structure](#directory-structure)  
    - [Schema](#schema)  
6. [Queries](#queries)  
    - [Query 1](#query-1)  
    - [Query 2](#query-2)  
    - [Query 3](#query-3)  
    - [Query 4](#query-4)  
    - [Query 5](#query-5)  
    - [Query 6](#query-6)  
    - [Query 7](#query-7)  
    - [Query 8](#query-8)  

> [!NOTE]  
> All roleplaying instructions, rubric requirements, and Starter Code (with  
> database and csv's) are provided by 2U/edX as part of their educational  
> package provided with paid entry into the class.  

## Introduction  
It’s been two weeks since I was hired as a new data engineer at Pewlett Hackard (a fictional company).  
My first major task is to do a research project about people whom the company employed during the 1980s  
and 1990s. All that remains of the employee database from that period are six CSV files.  

For this project, I have to design the tables to hold the data from the CSV files, import the CSV files  
into a SQL database, and then answer questions about the data. That is, I'll perform data modeling, data  
engineering, and data analysis, respectively.  


## Challenge Overview  
This challenge is interesting in that the most difficult part was the overall Schema creation and data  
modeling. While the querying was interesting, it didn't challenge my thinking the same was the schema  
creation did, and likely because by doing that work I saved myself a lot of trouble when querying.  

Through the completion of this challenge, I learned how strongly good data modeling depends on context.  
Understanding how the data was decided to be inputted, whether more data will ever come in, and how  
the relationships were envision when the data was originally written. Deciding whether those factors  
are important and should be retained, whether they are erroneous/irrelevant, and what can be changed  
to enhance database efficiency is what was truly challenging for me.


[:arrow_up: Return to TOC](#table-of-contents)  


## Entity Relationship Diagram (ERD)  

### sql-challenge_db DataBase Relationships  

<img src="EmployeeSQL/ERD.jpg" alt="ERD_sql_challenge" width="900" height="650">  


[:arrow_up: Return to TOC](#table-of-contents)  


## Summary Breakdowns:  
```
- Repository Creation and Directory Setup  
- CSV/Pandas DataFrame Analysis of Data  
- Entity Relationship Diagram  
   - Conceptual Relationships  
   - Logical Relationships  
   - Physical Relationships  
- Schema Setup/Initialization  
- CSV File Imports (6) Into New Tables  
- SQL Queries  
```  

[:arrow_up: Return to TOC](#table-of-contents)  


## Setup and Usage  
### Prerequisites & Instructions  
- Written in | **SQL:2023 (ISO/IEC 9075:2023)**  
- DBMS used | **PostgreSQL 17.2**  

1. Install PostgreSQL if not already done (ensure version is compatible with *17.2*)  
2. Clone this repository  
3. Create DataBase sql-challenge_db if it doesn't exist  
4. Connection to DataBase should be automatic, create a new `query tool window` or session  
5. Run the `schema.sql` file in a query window to setup/restart tables  
6. Import CSV Files from `sql-challenge_db/EmployeeSQL/data` directory into the corresponding tables  
7. Open the `queries.sql` file in a query window and run block-by-block (be sure to start with the `SET datestyle = 'Postgres, MDY';` line)  

[:arrow_up: Return to TOC](#table-of-contents)  


### Directory Structure  
```  
sql-challenge/  
|  
|—- EmployeeSQL/  
|   |—- data/  
|   |   —- departments.csv  
|   |   -- dept_emp.csv  
|   |   -- dept_manager.csv  
|   |   -- employees.csv  
|   |   -- salaries.csv  
|   |   -- titles.csv  
|   |  
|   |—- queries.sql  
|   |-- schema.sql  
```  
This structure ensures all inputs are organized within their respective folders.  

[:arrow_up: Return to TOC](#table-of-contents)  


### Schema  
**Main tables are: <ins>departments</ins>, <ins>titles</ins>, and <ins>employees</ins>**  
1. <ins>departments</ins>  
   - dept_no CHAR(4)  
   - dept_name VARCHAR(18)  
2. <ins>titles</ins>  
   - title_id CHAR(5)  
   - title VARCHAR(18)  
3. <ins>employees</ins>  
   - emp_no INT  
   - emp_title_id CHAR(5)  
   - birth_date DATE  
   - first_name VARCHAR(14)  
   - last_name VARCHAR(16)  
   - sex CHAR(1)  
   - hire_date DATE  
4. dept_emp  
   - emp_no INT  
   - dept_no CHAR(4)  
5. dept_manager  
   - dept_no CHAR(4)  
   - emp_no INT  
6. salaries  
   - emp_no INT  
   - salary INT  

[:arrow_up: Return to TOC](#table-of-contents)  


## Queries  

### Query 1  

```sql  
-- List the employee number, last name, first name, sex, and salary of each employee
SELECT
   e.emp_no
   ,e.last_name
   ,e.first_name
   ,e.sex
   ,s.salary
FROM
   employees e
INNER JOIN
   salaries s
ON
   e.emp_no = s.emp_no
;
```  

### Query 2  

```sql  
-- List the first name, last name, and hire date for the employees who were hired in 1986.
SELECT
   first_name
   ,last_name
   ,hire_date
FROM
   employees
WHERE
   EXTRACT(YEAR FROM hire_date) = 1986
;
```  

### Query 3  

```sql  
-- List the manager of each department along with their department number, department name, 
-- employee number, last name, and first name.
SELECT
   m.dept_no
   ,d.dept_name
   ,m.emp_no
   ,e.last_name
   ,e.first_name
FROM
   dept_manager m
INNER JOIN
   departments d
ON
   m.dept_no = d.dept_no
INNER JOIN
   employees e
ON
   m.emp_no = e.emp_no
;
```  

### Query 4  

```sql  
-- List the department number for each employee along with that employee’s employee number, 
-- last name, first name, and department name.
SELECT
   de.dept_no
   ,de.emp_no
   ,e.last_name
   ,e.first_name
   ,d.dept_name
FROM
   dept_emp de
INNER JOIN
   employees e
ON
   de.emp_no = e.emp_no
INNER JOIN
   departments d
ON
   de.dept_no = d.dept_no
;
```  

### Query 5  

```sql  
-- List first name, last name, and sex of each employee whose first name is Hercules and whose 
-- last name begins with the letter B.
SELECT
   first_name
   ,last_name
   ,sex
FROM
   employees
WHERE
   first_name = 'Hercules' AND
   last_name LIKE 'B%'
;
```  

### Query 6  

```sql  
-- List each employee in the Sales department, including their employee number, last name, and 
-- first name.
SELECT
   d.emp_no
   ,e.last_name
   ,e.first_name
FROM
   dept_emp d
INNER JOIN
   employees e
ON
   d.emp_no = e.emp_no
WHERE 
   d.dept_no = (
   SELECT dept_no
   FROM departments
   WHERE dept_name = 'Sales')
ORDER BY d.emp_no ASC
;
```  

### Query 7  

```sql  
-- List each employee in the Sales and Development departments, including their employee number, 
-- last name, first name, and department name.
SELECT
   de.emp_no
   ,e.last_name
   ,e.first_name
   ,d.dept_name
FROM
   dept_emp de
INNER JOIN
   employees e
ON
   de.emp_no = e.emp_no
INNER JOIN
   departments d
ON
   de.dept_no = d.dept_no
WHERE
   d.dept_name = 'Sales' OR
   d.dept_name = 'Development'
;
```  

### Query 8

```sql  
-- List the frequency counts, in descending order, of all the employee last names 
-- (that is, how many employees share each last name).
SELECT
   last_name
   ,COUNT(last_name) frequency
FROM
   employees
GROUP BY last_name
ORDER BY frequency DESC
;
```  

[:arrow_up: Return to TOC](#table-of-contents)  
#EOF

# SQL Alchemy Challenge - SurfsUp  
`Module 10`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
`By Neel Kumar Agarwal`  

## Table of Contents  
1. [Introduction](#introduction)  
2. [Setup & Usage](#setup--usage)  
    - [Prerequisites](#prerequisites)
    - [Instructions](#instructions)  
    - [Directory Structure](#directory-structure)  
3. [Challenge Overview](#challenge-overview)  
    - [Part 1](#part-1-analyzeexplore-data)  
    - [Part 2](#part-2-design-climate-app)  
4. [Queries](#queries)  
    - [Static](#static)  
        + [Precipitation (Inches)](#precipitation-inches---most-recent-year)
        + [All Stations](#all-observing-stations-with-station-info)
        + [Temperature Data (Most Active Station)](#temperature-data---most-recent-yearactive-station)
    - [Dynamic](#dynamic)  
        + [Temperature Data (Single Parameter)](#temperature-data---single-param)
        + [Temperature Data (Dual Parameter)](#temperature-data---dual-param)

> [!NOTE]  
> All roleplaying instructions, rubric requirements, and Starter Code (with  
> database and csv's) are provided by 2U/edX as part of their educational  
> package provided with paid entry into the class.  


## Introduction  
Congratulations to me! I've decided to treat myself to a long holiday vacation in Honolulu, Hawaii. To help with my trip planning, I'm going to do a climate analysis about the area.  
In order to do so, I'll first use Jupyter Notebooks to write/run code in isolated code blocks so that I can experiment with my `SQL Alchemy` abstraction layer and ensure that I perform my queries correctly. I'll also do some basic analysis by using `Pandas` and `Matplotlib` to get a better grasp on my data and and provide a visual aid. Jupyter Notebook is excellent for testing purposes as it allows for easy running and printing of code blocks. This also means that after I ensure it's veracity, I can copy and paste desired queries inside of a regular python (.py) file to run as an executable!  
That serves the purposes of this project very well, as after I finish exploring and doing my analysis of Hawaii's climate data I can create an application in a python file using `Flask` and `SQL Alchemy` to create APIs! `Flask` will allow for the exportation of python functions into specific webpage softlink routes, which can be filled with `SQL` queries performed with `SQL Alchemy`, as well as `HTML` and `CSS` code for any explanation or styling. Now I'm prepared for my vacation!  



## Setup & Usage  


### Prerequisites  
Python 3.x  
   - Standard libraries: `datetime` (included with Python)  
   - Non-standard library: `pandas`, `numpy`, `matplotlib`, `sqlalchemy`, and `Flask`  
   - IDE that supports Jupyter Notebooks with Python  
   - DBMS: **SQLite 3.49.0**  by way of --> **SQL Alchemy**  

[:arrow_up: Return to TOC](#table-of-contents)  


### Instructions  

1. Clone this repository and open it on your local device.  
2. For `climate_analysis.ipynb` (Part 1):  
    - Open `climate_analysis.ipynb` in your IDE and run all cells.  
    - (Better viewing experience by using the 'Outline' tab in VSCode)  
    - If the necessary dependencies aren't found, please install using one of the following methods (however pip is preferred):  
        - `pip install <missing_library>`  
        - `conda install <missing_library>` (use the channel of your choice if multiple are found)  
3. For `app.py` (Part 2):  
    - Open `app.py` using an integrated terminal and run using your python compiler (Version 3.x)  
    - By use of `Flask` a local only webpage will be instantiated on a server on your computer  
    - Perform any of the listed API calls using the webpage directly or by using a `requests.get()` method in python (as well in other languages that offer API extensions).  
4. Enjoy!  

[:arrow_up: Return to TOC](#table-of-contents)  



### Directory Structure  
```bash  
sqlalchemy-challenge/  
|  
|—- SurfsUp/  
|   |—- Resources/  
|   |   —- hawaii_measurements.csv  
|   |   -- hawaii_stations.csv  
|   |   -- hawaii.sqlite  
|   |  
|   |—- app.py  
|   |-- climate_analysis.ipynb  
```  
This structure ensures all inputs are organized within their respective folders.  

[:arrow_up: Return to TOC](#table-of-contents)  



## Challenge Overview  
The real purpose of this assignment is to explore using these technologies and methods in conjunction with each other, but within the scope of the project the purpose is the creation of multiple APIs that allow for calling to retrieve live JSON representation of queried data.  
Queries were performed differently than how they were likely expected, using a `sessionmaker()` binding to the engine and `with as` to create short-scoped session context managers through the application to open and close sessions as quickly as possible. They are personally preferred here as closing sessions throughout the experimentation of the Jupyter Notebook is easy to forget with isolated cell block styling.  


### Part 1: Analyze/Explore Data  
First off, I'll need to use Python and SQLAlchemy to do a basic climate analysis and data exploration of my climate database. Specifically, I'll use SQLAlchemy's ORM to perform queries, Pandas for easy manipulation, and Matplotlib for visualization.  
The following list outlines steps taken to perform exploration and analysis:  

1. Use SQLAlchemy method create_engine() to connect to the SQLite database.  
2. Use SQLAlchemy method automap_base() to reflect tables into classes and save references to the classes.  
3. Link Python to the database by creating SQLAlchemy sessions.  
4. Perform a precipitation analysis and station analysis...  

[:arrow_up: Return to TOC](#table-of-contents)  


### Part 2: Design Climate App  
The second part of this project is to create a functioning webpage based API that can be called  
like a normal API. This part actually combines the use of general use Python, SQL Alchemy, HTML, and CSS  
to create retrievable JSON objects. The following are the Flask App routes that will be created.  

1. **Route: /**  
    - Create a Homepage at the base route.  
2. **Route: /api/v1.0/precipitation**  
    - Convert the query results from the precipitation analysis to a dictionary using the date and precipitation as key: value pairs and returns the JSON.  
3. **Route: /api/v1.0/stations**  
    - Returns JSON of stations from the dataset.  
4. **Route: /api/v1.0/tobs**  
    - Queries dates/temperatures for the most active station of the prior year and returns the JSON.  
5. **Route: /api/v1.0/&lt;start&gt;**  
    - Returns JSON of temperature minimum, average, and maximum for a given starting date, which will end at the end of the database.  
6. **Route: /api/v1.0/&lt;start&gt;/&lt;end&gt;**  
    - Returns JSON of temperature minimum, average, and maximum for a given starting and ending date.  

[:arrow_up: Return to TOC](#table-of-contents)  


## Queries  
Query results will return with nested metadata separately from query results within the same JSON object. The `json_setup()` function is made to setup the dictionary with one line of code, giving them all similar structures.  

### Static  
#### Precipitation (Inches) - Most recent year  
```python  
def precipitation_query():
    '''Query for precipitation scores for last year of db'''
    last_date = app.config['DELTA_YEAR']
    sel = [
        measurement.date
        ,measurement.prcp
    ]
    with Session() as session: # Precipitation Scores
        data = session.query(*sel
            ).filter(measurement.date >= last_date
            ).all()
    data_nest = []
    for date, prcp in data:
        data_nest.append({date: prcp})
    json_ready = json_setup(
        route_prcp
        ,data_nest
        ,desc = 'Precipitation scores for last year of data.'
    )
    return jsonify(json_ready)
```  
[:arrow_up: Return to TOC](#table-of-contents)  


#### All observing Stations (with station info)  
```python  
def stations_query():
    '''Query for full list of stations'''
    sel = [
        station.station
        ,station.name
        ,station.latitude
        ,station.longitude
        ,station.elevation
    ]
    with Session() as session:
        data = session.query(*sel).all()
    data_nest = []
    for id, name, latitude, longitude, elevation in data:
        data_dict = {}
        data_dict['id'] = id
        data_dict['name'] = name
        data_dict['lat'] = latitude
        data_dict['lng'] = longitude
        data_dict['elevation'] = elevation
        data_nest.append(data_dict)
    json_ready = json_setup(
        route_stations
        ,data_nest
        ,desc = 'Full list of observation stations.'
    )
    return jsonify(json_ready)
```  
[:arrow_up: Return to TOC](#table-of-contents)  


#### Temperature data - Most Recent Year/Active Station  
```python  
def tobs_query():
    '''Query for last year of temp data for most active station'''
    last_date = app.config['DELTA_YEAR']
    station_count = func.count(measurement.station)
    sel = [
        measurement.station
        ,station_count
    ]
    with Session() as session:
        data = session.query(*sel
            ).group_by(measurement.station
            ).order_by(station_count.desc()
            ).first()
    most_active = data[0]
    sel = [
        measurement.date
        ,measurement.tobs
    ]
    with Session() as session:
        data = session.query(*sel
            ).filter(measurement.station == most_active
            ).filter(measurement.date >= last_date
            ).all()
    data_nest = []
    for date, tobs in data:
        data_dict = {}
        data_dict['date'] = date
        data_dict['tobs'] = tobs
        data_nest.append(data_dict)
    json_ready = json_setup(
        route_tobs
        ,data_nest
        ,desc = 'Last year of temperature data for most active station.'
    )
    return jsonify(json_ready)
```  
[:arrow_up: Return to TOC](#table-of-contents)  


### Dynamic  
The following script the function shared between the two dynamic calls allowed for this API.  

```python  
def temp_byDate(start = None, end = None):
    sel = [
        func.min(measurement.tobs)
        ,func.avg(measurement.tobs)
        ,func.max(measurement.tobs)
    ]
    if start and (not end):
        with Session() as session:
            data = session.query(*sel
                ).filter(measurement.date >= start)
    elif start and end:
        with Session() as session:
            data = session.query(*sel
                ).filter(measurement.date >= start
                ).filter(measurement.date <= end)
    else:
        print('Sorry, this API call is not supported yet. ERROR 404')
        abort(404)
    data = data.first()
    data_nest = {
        'TMIN': data[0]
        ,'TAVG': data[1]
        ,'TMAX': data[2]
    }
    return data_nest
```  
[:arrow_up: Return to TOC](#table-of-contents)  


#### Temperature data - Single Param  
```python  
def temp_filter_single(start):
    data_nest = temp_byDate(start)
    json_ready = json_setup(
        route_start
        ,data_nest
        ,desc = 'Basic temperature stats (min, avg, max) for a specified starting date, ends at most recent date.'
        ,params = {'start_date': start, 'end_date': None}
    )
    return jsonify(json_ready)
```  
[:arrow_up: Return to TOC](#table-of-contents)  


#### Temperature data - Dual Param  
```python  
def temp_filter_double(start, end):
    data_nest = temp_byDate(start, end)
    json_ready = json_setup(
        route_end
        ,data_nest
        ,desc = 'Basic temperature stats (min, avg, max) for a specified start and end date.'
        ,params = {'start_date': start, 'end_date': end}
    )
    return jsonify(json_ready)
```  
[:arrow_up: Return to TOC](#table-of-contents)  

#EOF

# Webscraper Challenge - MarsUpdate
`Module 5`  
`EdX(2U) & UT Data Analytics and Visualization Bootcamp`  
`Cohort UTA-VIRT-DATA-PT-11-2024-U-LOLC`  
**By:** Neel Kumar Agarwal  

## Table of Contents
1. [Introduction](#introduction)  
2. [Challenge Overview](#challenge-overview)  
3. [Deliverables](#deliverables)  
4. [Setup and Usage](#setup-and-usage)  
   - [Prerequisites](#prerequisites)  
   - [Instructions](#instructions)  
   - [Limitations](#limitations)  
5. [Files and Directory Structure](#files-and-directory-structure)  
6. [Expected Results](#expected-results)  
7. [Analysis](#analysis)  
   - [Scraping Titles and Text (Part 1)](#scraping-titles-and-text-part-1)  
   - [Scraping and Analyzing Weather Data (Part 2)](#scraping-and-analyzing-weather-data-part-2)  
   - [Embedded Analysis Discussion](#embedded-analysis)  
8. [Citations / References](#citations--references)  

---

## Introduction
In this challenge, we use **Splinter** (for browser automation) and **BeautifulSoup** (for HTML parsing) to:
1. **Scrape Titles and Preview Text** of the latest Mars news articles.
2. **Scrape and Analyze Mars Weather Data** (temperature and pressure readings) from a static webpage.  

Both tasks are performed in Jupyter notebooks so that code, outputs, and analytical commentary can be intermixed in one file.

---

## Challenge Overview
1. **Deliverable 1**: Scrape Titles and Preview Text from Mars News  
   - Access a static Mars news site using Splinter.  
   - Parse the HTML elements with BeautifulSoup.  
   - Collect the article titles and previews, store them in dictionaries, then print your final list.  

2. **Deliverable 2**: Scrape and Analyze Mars Weather Data  
   - Access a static Mars facts site using Splinter.  
   - Parse the HTML table to extract data (Martian day, temperature, atmospheric pressure, etc.).  
   - Convert to a Pandas DataFrame, clean the data types, produce summary statistics, and visualize results.  
   - Export your final DataFrame to a CSV file.  
   - Provide short commentary on the findings for the coldest months, pressure fluctuations, and approximate length of a Martian year.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Deliverables
1. **part_1_mars_news.ipynb**  
   - A Jupyter notebook for scraping the Mars news site for article titles and teasers.  
   - Stores results in Python data structures and prints them out.  

2. **part_2_mars_weather.ipynb**  
   - A Jupyter notebook that scrapes the Mars temperature/pressure data site, analyzes it in Pandas, and visualizes results.  
   - Includes **inline commentary** on the results.  

3. **README.md**  
   - Explains the project goals, usage instructions, and final outcomes.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Setup and Usage

### Prerequisites
- **Python 3.x**  
- **Anaconda** or another environment manager that supports Jupyter notebooks.  
- **Pandas** and **Matplotlib**: install via `pip install pandas matplotlib` or `conda install pandas matplotlib`.  
- **Splinter** library: `pip install splinter`  
- **BeautifulSoup**: `pip install beautifulsoup4`  
- A compatible **web driver** for Splinter (e.g., **ChromeDriver**) that is accessible via PATH or an absolute path.


### Instructions
1. **Clone this repository** (or download its `.ipynb` files).  
2. **Create or activate a conda environment**:
   ```bash
   conda create --name mars_env python=3.9
   conda activate mars_env
   ```
3. **Install dependencies**:
   ```bash
   pip install splinter beautifulsoup4 pandas matplotlib
   ```
4. **Launch Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```
5. **Open `part_1_mars_news.ipynb`** and run all cells:
   - This will automate a browser session to the Mars News site.  
   - It will parse the HTML, extract article titles and previews, and display them in your notebook output cells.

6. **Open `part_2_mars_weather.ipynb`** and run all cells:
   - This will automate a browser session to the Mars facts site.  
   - It will scrape the data table, load into a Pandas DataFrame, visualize monthly temperature/pressure, and produce final CSV output (`mars_data.csv`).  
   - Check the markdown cells labeled for “Analysis” to see commentary on findings.


### Limitations
- **Static scraping**: Only tested against the provided static pages. Changes to these pages may break the script.  
- **Driver setup**: Must ensure ChromeDriver or your chosen driver is in PATH.  
- **Notebook**: Code must be run in an environment that supports Jupyter Notebook or JupyterLab.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Files and Directory Structure
```
webscraper-challenge/
│
├── MarsUpdate/
│   │
│   ├── Figures/ (Optionally created)
│   │   ├── avgTemp_ByMonth.png
│   │   ├── avgAtm_byMonth.png
│   │   └── temp_byTime.png
│   │
│   ├──mars_data.csv (Exported by part_2_mars_weather.ipynb)
│   │
│   ├── part_1_mars_news.ipynb
│   └── part_2_mars_weather.ipynb
│
├── .gitignore  
├── README.md
```

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Expected Results
**part_1_mars_news.ipynb**  
- Displays a list of dictionaries where each dictionary has:
   ```python
   {
      'title': 'Some Mars Article Title',
      'preview': 'Preview text snippet for that article...'
   }
   ```

**part_2_mars_weather.ipynb**  
- Displays a **Pandas DataFrame** of Martian weather data in the notebook.  
- Produces bar charts summarizing average min temperature and average pressure by month.  
- Shows a line plot of daily min temps over time to estimate the length of a Martian year.  
- Outputs `mars_data.csv` as a clean data file.  
- Markdown cells contain commentary on the average minimum temperatures per moth, the average pressures per month, and rough calculation for how many Earth days are in a Martian year.  
- Bar Charts:
   + ![avgTemp_ByMonth](MarsUpdate/Figures/avgTemp_byMonth.png)  
   + ![avgAtm_ByMonth](MarsUpdate/Figures/avgAtm_byMonth.png)  
   + ![temp_byTime](MarsUpdate/Figures/temp_byTime.png)

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Analysis

### Scraping Titles and Text (Part 1)
**Brief Summary**  
1. **Site Visitation**: Splinter automates a Chrome browser to go to the static Mars News site.  
2. **HTML Parsing**: BeautifulSoup locates each article container.  
3. **Data Extraction**: Title and preview snippet are stored in a dictionary.  
4. **Output**: The final list is displayed in the notebook.


### Scraping and Analyzing Weather Data (Part 2)
**Brief Summary**  
1. **Site Visitation**: Splinter visits the static Mars Temperature data page.  
2. **Table Parsing**: BeautifulSoup extracts rows (`<tr>`) and cells (`<td>`).  
3. **DataFrame Creation**:  
   - Columns: `id`, `terrestrial_date`, `sol`, `ls`, `month`, `min_temp`, `pressure`.  
   - Converted columns to correct data types.  
4. **Exploratory Analysis**:  
   - Summary stats.  
   - Visual analysis with bar charts.  
   - Identify the coldest/hottest months and lowest/highest pressure months.  
   - Identify average pressures per month and statistically relevant data.  
   - Estimate Earth days in a Martian year using temperature data.  
5. **CSV Export**: The final dataset is written to `mars_data.csv`.


### Embedded Analysis
- **Coldest/Hottest Months:**  
   > The minimum temperatures for Mars are quite low in comparison to our standard Earth temperatures, but they do follow some basic rules as per their planet. Primarily, the temperatures fluctuate from approximately -83.3 degrees celsius to -68.3 degrees celsius, centering around -75.5 degrees. While there are only 12 data points for the 12 months in the graph, those points come from aggregating by averages, and thus the standard deviation holds a lot of meaning in that the values are not incredibly spread far apart. From the graph, it can be seen that the first 6 months are in fact the hottest in terms of minimum temperatures. It does also seem to operate in a single cycle. 

- **Lowest/Highest Pressure Months:**  
   > Atmospheric pressure actually functions in two cycles, with a much larger dispersement in terms of standard deviation (55.5), however atmospheric pressure values are normally quite large. In this case, Mars' pressure extremes are approximately 745 and 913.3, with a mean of 841.5. The lowest low occurs in month 6, whereas the highest high occurs in month 9. 

- **Length of a Martian Year:**  
   > Finding the length of a Martian year by the guided/described method uses the visual aid of plotting minimum temperatures with consecutively counted days terrestrial (Earth) days. By matching the cyclical pattern in Mars' temperature cycles it's possible to calculate an approximate Martian year in Earth days.  If taken like a typical wave, the year is simply one wavelength, or the total days divided by the number of wavelengths. In this case, it could either be found by visually finding one single cycle which looks to be around 625 terrestrial days. However, for an average, the total axis length could be taken as 1875 / 3, which would also come out at 625 terrestrial days! Thus validating the result a little more.

[:arrow_up: Return to TOC](#table-of-contents)  

---

## Citations / References
- **Starter Data**: Provided via edX/2U’s Module 11 resources.  
- **README.md**: Created using OpenAI's [ChatGPT LLM](https://www.chatgpt.com), trained using prior READMEs from project owner and sole contributor's repository [Neel Agarwal (Neelka96)](https://www.github.com/Neelka96), the two deliverables, and the provided rubric given by edX/2U  
- **BeautifulSoup Documentation**: [https://www.crummy.com/software/BeautifulSoup](https://www.crummy.com/software/BeautifulSoup)  
- **Splinter Documentation**: [https://splinter.readthedocs.io/en/latest/](https://splinter.readthedocs.io/en/latest/)  
- **Chrome WebDriver**: [https://chromedriver.chromium.org](https://chromedriver.chromium.org)  

[:arrow_up: Return to TOC](#table-of-contents)  
#EOF

